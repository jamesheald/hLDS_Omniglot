{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57469386-cfb5-4f1c-a58b-5b9df4965138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random, lax, jit, vmap, value_and_grad\n",
    "from jax.tree_util import tree_map\n",
    "from jax.nn import initializers\n",
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "from flax import linen as nn\n",
    "\n",
    "import optax\n",
    "from flax.training import checkpoints\n",
    "from flax.training.early_stopping import EarlyStopping\n",
    "\n",
    "import time\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "658e9930-284e-4f27-8f32-6f4f67679b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    n_loops_top_layer: int\n",
    "    x_dim_top_layer: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # CNN based on End-to-End Training of Deep Visuomotor Policies\n",
    "        x = nn.Conv(features = 64, kernel_size = (7, 7))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features = 32, kernel_size = (5, 5))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features = 32, kernel_size = (5, 5))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = x.reshape((x.shape[0], -1)) # flatten\n",
    "        x = nn.Dense(features = 64)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features = 40)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features = 40)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features = (self.n_loops_top_layer + self.x_dim_top_layer) * 2)(x)\n",
    "        \n",
    "        # # CNN based on deepmimic\n",
    "        # x = nn.Conv(features = 16, kernel_size = (8, 8))(x)\n",
    "        # x = nn.relu(x)\n",
    "        # x = nn.Conv(features = 32, kernel_size = (4, 4))(x)\n",
    "        # x = nn.relu(x)\n",
    "        # x = nn.Conv(features = 32, kernel_size = (4, 4))(x)\n",
    "        # x = nn.relu(x)\n",
    "        # x = x.reshape((x.shape[0], -1)) # flatten\n",
    "        # x = nn.Dense(features = 64)(x)\n",
    "        # x = nn.relu(x)\n",
    "        # x = nn.Dense(features = 1024)(x)\n",
    "        # x = nn.relu(x)\n",
    "        # x = nn.Dense(features = 512)(x)\n",
    "        # x = nn.relu(x)\n",
    "        # x = nn.Dense(features = (self.n_loops_top_layer + self.x_dim_top_layer) * 2)(x)\n",
    "        \n",
    "        # mean and log variances of Gaussian distribution over latents\n",
    "        z_mean, z_log_var = np.split(x, 2, axis = 1)\n",
    "        \n",
    "        return z_mean, z_log_var\n",
    "    \n",
    "class pen_actions_readout(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # linear layer\n",
    "        outputs = nn.Dense(3)(x)\n",
    "\n",
    "        # pen velocity in x and y directions\n",
    "        d_xy = outputs[:2]\n",
    "\n",
    "        # log probability that the pen is down (actively drawing)\n",
    "        pen_down_log_p = nn.log_sigmoid(outputs[2])\n",
    "    \n",
    "        return d_xy, pen_down_log_p\n",
    "    \n",
    "class sample_latents():\n",
    "    \n",
    "    def __init__(self, n_loops_top_layer):\n",
    "        self.n_loops_top_layer = n_loops_top_layer\n",
    "    \n",
    "    def __call__(self, results, params, hyperparams, key):\n",
    "\n",
    "        # sample the latents\n",
    "        z = diag_Gaussian_sample(results['z_mean'], results['z_log_var'], hyperparams, key)\n",
    "\n",
    "        # split into latents that determine the top-layer alphas (z1) and initial state (z2)\n",
    "        z1, z2 = np.split(z, [self.n_loops_top_layer], axis = 1)\n",
    "\n",
    "        return nn.activation.softmax(z1 / params['dynamics']['t'][0], axis = 1), z2\n",
    "    \n",
    "def update_pen_position(pen_xy, d_xy, hyperparams):\n",
    "    \n",
    "    # candidate new pen position\n",
    "    pen_xy = pen_xy + d_xy\n",
    "    \n",
    "    # align pen position relative to centre of canvas\n",
    "    pen_xy = pen_xy - hyperparams['image_dim'] / 2\n",
    "    \n",
    "    # transform canvas boundaries to -/+ 5\n",
    "    pen_xy = pen_xy * 2 / hyperparams['image_dim'] * 5\n",
    "    \n",
    "    # squash pen position to be within canvas boundaries\n",
    "    pen_xy = nn.sigmoid(pen_xy)\n",
    "    \n",
    "    # transform canvas boundaries back to their original values\n",
    "    pen_xy_new = pen_xy * hyperparams['image_dim']\n",
    "    \n",
    "    return pen_xy_new\n",
    "\n",
    "def compute_alphas(W, x, b, t):\n",
    "    \n",
    "    return nn.activation.softmax( (W @ x + b) / t, axis = 0)\n",
    "\n",
    "def compute_inputs(W, x):\n",
    "    \n",
    "    return W @ x\n",
    "\n",
    "def update_state(A, x, alphas, u, dt):\n",
    "    \n",
    "    return x + (np.sum(alphas[:, None, None] * A, axis = 0) @ x + u) * dt\n",
    "\n",
    "def decode_one_step(carry, inputs):\n",
    "    \n",
    "    params, hyperparams, A, x, pen_xy = carry\n",
    "    top_layer_alphas = inputs\n",
    "    \n",
    "    # compute the alphas\n",
    "    alphas = jax.tree_map(compute_alphas, params['dynamics']['W_a'], x[:2], params['dynamics']['b'], \n",
    "                          params['dynamics']['t'][1:])\n",
    "\n",
    "    # prepend the top-layer alphas\n",
    "    alphas.insert(0, top_layer_alphas)\n",
    "    \n",
    "    # compute the additive inputs\n",
    "    u = jax.tree_map(compute_inputs, params['dynamics']['W_u'], x[:2])\n",
    "    \n",
    "    # prepend the top-layer additive inputs\n",
    "    u.insert(0, x[0] * 0)\n",
    "\n",
    "    # update the states\n",
    "    x_new = jax.tree_map(update_state, A, x, alphas, u, hyperparams['dt'])\n",
    "\n",
    "    # readout the pen actions (pen velocity and pen down probability) from the state at the bottom layer\n",
    "    d_xy, pen_down_log_p = readout.apply(params['readout'], x = x_new[-1])\n",
    "    \n",
    "    # calculate the per-pixel bernoulli parameter\n",
    "    p_xy = per_pixel_bernoulli_parameter(params, hyperparams, pen_xy, pen_down_log_p)\n",
    "    \n",
    "    # update the pen position based on the pen velocity\n",
    "    pen_xy_new = update_pen_position(pen_xy, d_xy, hyperparams)\n",
    "\n",
    "    carry = params, hyperparams, A, x_new, pen_xy_new\n",
    "    outputs = alphas, x_new, pen_xy_new, p_xy, pen_down_log_p\n",
    "    \n",
    "    return carry, outputs\n",
    "\n",
    "def decode(params, hyperparams, models, A, x0, T, z1, z2):\n",
    "\n",
    "    x0[0] = z2\n",
    "    pen_xy0 = hyperparams['image_dim'] / 2 # initialise pen in centre of canvas\n",
    "\n",
    "    # decoder = lambda state, inputs: decode_one_step(params, hyperparams, models, A, state = state, inputs = inputs)\n",
    "    # _, (alphas, x, pen_xy, p_xy_t, pen_down_log_p) = lax.scan(decoder, (x0, pen_xy0), np.repeat(z1[None,:], T, axis = 0))\n",
    "    \n",
    "    # _, _, readout = models\n",
    "    carry = params, hyperparams, A, x0, pen_xy0\n",
    "    inputs = np.repeat(z1[None,:], T, axis = 0)\n",
    "    \n",
    "    _, (alphas, x, pen_xy, p_xy_t, pen_down_log_p) = lax.scan(decode_one_step, carry, inputs)\n",
    "    \n",
    "    return {'alphas': alphas,\n",
    "            'x0': x0,\n",
    "            'x': x,\n",
    "            'pen_xy0': pen_xy0,\n",
    "            'pen_xy': pen_xy,\n",
    "            'p_xy_t': p_xy_t,\n",
    "            'pen_down_log_p': pen_down_log_p}\n",
    "\n",
    "batch_decode = vmap(decode, in_axes = (None, None, None, None, None, None, 0, 0))\n",
    "\n",
    "def encode(models, params, data):\n",
    "\n",
    "    encoder, _, _ = models\n",
    "    z_mean, z_log_var = encoder.apply(params['encoder'], x = data)\n",
    "\n",
    "    return {'z_mean': z_mean, \n",
    "            'z_log_var': z_log_var}\n",
    "    \n",
    "def losses(params, hyperparams, models, data, x0, T, kl_weight, key):\n",
    "    \n",
    "    # pass the data through the encoder\n",
    "    results_encode = encode(models, params, data)\n",
    "    \n",
    "    # sample the latent variables from the approximate posterior\n",
    "    _, z_sampler, _ = models\n",
    "    z1, z2 = z_sampler(results_encode, params, hyperparams, key)\n",
    "\n",
    "    # pass the latent variables through the decoder\n",
    "    A = construct_dynamics_matrix(params, hyperparams)\n",
    "    results_decode = batch_decode(params, hyperparams, models, A, x0, T, z1, z2)\n",
    "    \n",
    "    results = results_encode | results_decode\n",
    "\n",
    "    # cross entropy given latent variables\n",
    "    cross_entropy = batch_cross_entropy_loss(params, hyperparams, results['p_xy_t'], data).mean()\n",
    "    \n",
    "    # KL divergence between the approximate posterior and prior over the latents\n",
    "    mu_0 = 0\n",
    "    log_var_0 = params['prior_z_log_var']\n",
    "    mu_1 = results['z_mean']\n",
    "    log_var_1 = results['z_log_var']\n",
    "    kl_loss_prescale = batch_KL_diagonal_Gaussians(mu_0, log_var_0, mu_1, log_var_1, hyperparams).mean()\n",
    "    kl_loss = kl_weight * kl_loss_prescale\n",
    "    \n",
    "    loss = cross_entropy + kl_loss\n",
    "    \n",
    "    all_losses = {'total': loss, 'cross_entropy': cross_entropy, 'kl': kl_loss, 'kl_prescale': kl_loss_prescale}\n",
    "    \n",
    "    return loss, all_losses\n",
    "\n",
    "losses_jit = jit(losses, static_argnums = (2, 5))\n",
    "training_loss_grad = value_and_grad(losses_jit, has_aux = True)\n",
    "\n",
    "def construct_dynamics_matrix(params, hyperparams):\n",
    "    \n",
    "    def construct_P(P):\n",
    "        return P @ P.T\n",
    "    \n",
    "    def construct_S(U, V):\n",
    "        return U @ np.transpose(V, (0, 2, 1)) - V @ np.transpose(U, (0, 2, 1))\n",
    "    \n",
    "    def construct_A(L, P, S):\n",
    "        return (-L @ np.transpose(L, (0, 2, 1)) + S) @ P\n",
    "    \n",
    "    # positive semi-definite matrix P\n",
    "    P = jax.tree_map(construct_P, params['dynamics']['P'])\n",
    "\n",
    "    # skew symmetric matrix S\n",
    "    S = jax.tree_map(construct_S, params['dynamics']['S_U'], params['dynamics']['S_V'])\n",
    "\n",
    "    # dynamics matrix A (loops organised along axis 0)\n",
    "    A = jax.tree_map(construct_A, params['dynamics']['L'], S, P)\n",
    "\n",
    "    return A\n",
    "\n",
    "def stabilise_varance(log_var, hyperparams):\n",
    "    \"\"\"\n",
    "    var_min is added to the variances for numerical stability\n",
    "    \"\"\"\n",
    "    return np.log(np.exp(log_var) + hyperparams['var_min'])\n",
    "\n",
    "def diag_Gaussian_sample(mean, log_var, hyperparams, key):\n",
    "    \"\"\"\n",
    "    sample from a diagonal Gaussian distribution\n",
    "    \"\"\"\n",
    "    log_var = stabilise_varance(log_var, hyperparams)\n",
    "    \n",
    "    return mean + np.exp(0.5 * log_var) * random.normal(key, mean.shape)\n",
    "\n",
    "def log_Gaussian_kernel(x, mu, log_var, hyperparams):\n",
    "    \"\"\"\n",
    "    calculate the log likelihood of x under a diagonal Gaussian distribution\n",
    "    \"\"\"\n",
    "    log_var = stabilise_varance(log_var, hyperparams)\n",
    "    \n",
    "    return -0.5 * (x - mu)**2 / np.exp(log_var)\n",
    "\n",
    "def log_likelihood_diagonal_Gaussian(x, mu, log_var, hyperparams):\n",
    "    \"\"\"\n",
    "    calculate the log likelihood of x under a diagonal Gaussian distribution\n",
    "    \"\"\"\n",
    "    log_var = stabilise_varance(log_var, hyperparams)\n",
    "    \n",
    "    return np.sum(-0.5 * (log_var + np.log(2 * np.pi) + (x - mu)**2 / np.exp(log_var)))\n",
    "\n",
    "def per_pixel_bernoulli_parameter(params, hyperparams, pen_xy, pen_down_log_p):\n",
    "    \n",
    "    ll_p_x = log_Gaussian_kernel(pen_xy[0], hyperparams['x_pixels'], params['pen_log_var'], hyperparams)\n",
    "    ll_p_y = log_Gaussian_kernel(pen_xy[1], hyperparams['y_pixels'], params['pen_log_var'], hyperparams)\n",
    "    \n",
    "    p_xy_t = np.exp(ll_p_x[None,:] + ll_p_y[:,None] + pen_down_log_p)\n",
    "    \n",
    "    return p_xy_t\n",
    "\n",
    "def cross_entropy_loss(params, hyperparams, p_xy_t, data):\n",
    "    \n",
    "    # compute the smooth maximum of the per-pixel bernoulli parameter across time steps\n",
    "    p_xy = np.sum(p_xy_t * nn.activation.softmax(p_xy_t * hyperparams['smooth_max_parameter'], axis = 0), axis = 0)\n",
    "\n",
    "    # compute the logit for each pixel\n",
    "    logits = np.log(p_xy / (1 - p_xy))\n",
    "    \n",
    "    # compute the average cross entropy across pixels\n",
    "    cross_entropy = np.mean(optax.sigmoid_binary_cross_entropy(logits, data))\n",
    "\n",
    "    return cross_entropy\n",
    "\n",
    "batch_cross_entropy_loss = vmap(cross_entropy_loss, in_axes = (None, None, 0, 0))\n",
    "\n",
    "def KL_diagonal_Gaussians(mu_0, log_var_0, mu_1, log_var_1, hyperparams):\n",
    "    \"\"\"\n",
    "    KL(q||p), where q is posterior and p is prior\n",
    "    mu_0, log_var_0 is the mean and log variances of the prior\n",
    "    mu_1, log_var_1 is the mean and log variances of the posterior\n",
    "    var_min is added to the variances for numerical stability\n",
    "    \"\"\"\n",
    "    log_var_1 = stabilise_varance(log_var_1, hyperparams)\n",
    "    \n",
    "    return np.sum(0.5 * (log_var_0 - log_var_1 + np.exp(log_var_1 - log_var_0) \n",
    "                         - 1.0 + (mu_1 - mu_0)**2 / np.exp(log_var_0)))\n",
    "\n",
    "batch_KL_diagonal_Gaussians = vmap(KL_diagonal_Gaussians, in_axes = (None, None, 0, 0, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43d39ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_dynamics_parameters(hyperparams,  key):\n",
    "    \n",
    "    P = []\n",
    "    S_U = []\n",
    "    S_V = []\n",
    "    L = []\n",
    "    W_u = []\n",
    "    W_a = []\n",
    "    b = []\n",
    "    t = []\n",
    "    \n",
    "    n_layers = len(hyperparams['x_dim'])\n",
    "    for layer in range(n_layers):\n",
    "        \n",
    "        key, *subkeys = random.split(key, num = 7)\n",
    "        \n",
    "        n_loops = hyperparams['n_loops'][layer]\n",
    "        x_dim = hyperparams['x_dim'][layer]\n",
    "        \n",
    "        # parameters of layer-specific P\n",
    "        p = random.normal(subkeys[0], (x_dim, x_dim))\n",
    "        \n",
    "        # set trace of P @ P.T to x_dim\n",
    "        P.append(p * np.sqrt(x_dim / np.trace(p @ p.T)))\n",
    "        \n",
    "        # parameters of layer- and loop-specific S\n",
    "        u = random.normal(subkeys[1], (n_loops, x_dim, int(x_dim / n_loops)))\n",
    "        v = random.normal(subkeys[2], (n_loops, x_dim, int(x_dim / n_loops)))\n",
    "        \n",
    "        # set variance of elements of S to 1/(n_loops * x_dim)\n",
    "        s = u @ np.transpose(v, (0, 2, 1)) - v @ np.transpose(u, (0, 2, 1))\n",
    "        # f = 1 / np.linalg.norm(s, axis = (1, 2))[:, None, None] / n_loops # frobenius norm of each loop 1/n_loops\n",
    "        f = 1 / np.std(s, axis = (1, 2))[:, None, None] / np.sqrt(n_loops * x_dim)\n",
    "        S_U.append(u * np.sqrt(f))\n",
    "        S_V.append(v * np.sqrt(f))\n",
    "\n",
    "        # parameters of layer- and loop-specific L\n",
    "        Q, _ = np.linalg.qr(random.normal(subkeys[3], (x_dim, x_dim)))\n",
    "        L_i = np.split(Q * np.sqrt(n_loops), n_loops, axis = 1)\n",
    "        L.append(np.stack(L_i, axis = 0))\n",
    "\n",
    "        # parameters of the mapping from hidden states to alphas, alphas = softmax(W @ x + b, temperature)\n",
    "        if layer != 0:\n",
    "            \n",
    "            # weights for additive inputs\n",
    "            std_W = 1 / np.sqrt(hyperparams['x_dim'][layer - 1])\n",
    "            W_u.append(random.normal(subkeys[4], (hyperparams['x_dim'][layer], hyperparams['x_dim'][layer - 1])) * std_W)\n",
    "            \n",
    "            # weights for modulatory factors\n",
    "            W_a.append(random.normal(subkeys[5], (n_loops, hyperparams['x_dim'][layer - 1])) * std_W)\n",
    "\n",
    "            # bias for modulatory factors\n",
    "            b.append(np.zeros((n_loops)))\n",
    "            \n",
    "        # temperature of layer-specific softmax function\n",
    "        t.append(1.0)\n",
    "\n",
    "    return {'P': P, \n",
    "            'S_U': S_U,\n",
    "            'S_V': S_V, \n",
    "            'L': L, \n",
    "            'W_u': W_u,\n",
    "            'W_a': W_a, \n",
    "            'b': b,\n",
    "            't': t}\n",
    "\n",
    "def initialise_hidden_states(hyperparams):\n",
    "              \n",
    "    # initialise the hidden states of the decoder model to zero (not learned)\n",
    "    n_layers = len(hyperparams['x_dim'])\n",
    "    init_states = []\n",
    "    for layer in range(n_layers):\n",
    "        init_states.append(np.zeros(hyperparams['x_dim'][layer]))\n",
    "        \n",
    "    return init_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1187743d-c012-46ba-9698-23fd82085055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random as rdm\n",
    "from sys import platform as sys_pf\n",
    "import matplotlib\n",
    "if sys_pf == 'darwin':\n",
    "    matplotlib.use(\"TkAgg\")\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def num2str(idx):\n",
    "    if idx < 10:\n",
    "        return '0'+str(idx)\n",
    "    return str(idx)\n",
    "\n",
    "def load_img(fn):\n",
    "    I = plt.imread(fn)\n",
    "    I = np.array(I,dtype=bool)\n",
    "    return I\n",
    "\n",
    "def load_motor(fn):\n",
    "    motor = []\n",
    "    with open(fn,'r') as fid:\n",
    "        lines = fid.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    for myline in lines:\n",
    "        if myline =='START': # beginning of character\n",
    "            stk = []\n",
    "        elif myline =='BREAK': # break between strokes\n",
    "            stk = np.array(stk)\n",
    "            motor.append(stk) # add to list of strokes\n",
    "            stk = [] \n",
    "        else:\n",
    "            arr = np.fromstring(myline,dtype=float,sep=',')\n",
    "            stk.append(arr)\n",
    "    return motor\n",
    "\n",
    "img_dir = '/Users/James/Dropbox/James MacBook/Guillaume/omniglot/omniglot/python/images_background'\n",
    "stroke_dir = '/Users/James/Dropbox/James MacBook/Guillaume/omniglot/omniglot/python/strokes_background'\n",
    "pth_i = os.path.join(img_dir, 'Sanskrit')\n",
    "pth_s = os.path.join(stroke_dir, 'Sanskrit')\n",
    "n_characters = len([s for s in os.listdir(pth_i) if 'character' in s])\n",
    "n_reps_per_character = 20\n",
    "training_data = onp.empty((n_characters * n_reps_per_character, 105, 105))\n",
    "for character in range(1):\n",
    "    \n",
    "    # get directories for this character\n",
    "    img_char_dir = os.path.join(pth_i, 'character' + num2str(character + 1))\n",
    "    stroke_char_dir = os.path.join(pth_s, 'character' + num2str(character + 1))\n",
    "    \n",
    "    # get base file name for this character\n",
    "    # print(os.listdir(img_char_dir)[0])\n",
    "    fn_example = os.listdir(img_char_dir)[0]\n",
    "    fn_base = fn_example[:fn_example.find('_')] \n",
    "    \n",
    "    for rep in range(1):\n",
    "        \n",
    "        fn_img = img_char_dir + '/' + fn_base + '_' + num2str(rep + 1) + '.png'\n",
    "        I = load_img(fn_img) == False # ensures letter pixels are 1\n",
    "        training_data[character * n_reps_per_character + rep,:,:] = I\n",
    "        \n",
    "        fn_stk = stroke_char_dir + '/' + fn_base + '_' + num2str(rep + 1) + '.txt'\n",
    "        # motor = load_motor(fn_stk)\n",
    "\n",
    "training_data = np.array(training_data)\n",
    "\n",
    "# # plot images and strokes\n",
    "# plt.imshow(training_data[0,:,:], cmap = 'binary')\n",
    "# plt.show()\n",
    "# for ex in range(1):\n",
    "#     for m in motor:\n",
    "#         plt.scatter(m[:,0],m[:,1], c = 'k')\n",
    "# plt.ylim(-105, 0)\n",
    "# plt.xlim(0, 105)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fca12b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly generate a PRNG key\n",
    "seed = 0\n",
    "key = random.PRNGKey(seed)\n",
    "\n",
    "# generate the required number of subkeys\n",
    "key, *subkeys = random.split(key, num = 5)\n",
    "\n",
    "image_height = 105\n",
    "image_width = 105\n",
    "validation_data = training_data[0:20,:,:]\n",
    "\n",
    "# primary hyperparameters\n",
    "hyperparams = {'image_dim': np.array(training_data.shape[1:]),\n",
    "               'x_dim': [20, 50, 200],\n",
    "               'alpha_fraction': 0.1,\n",
    "               'dt': [0.01, 0.01, 0.01],\n",
    "               'time_steps': 100,\n",
    "               'var_min': 1e-16,\n",
    "               'smooth_max_parameter': 1e3}\n",
    "\n",
    "# secondary hyperparameters (derived from primary hyperparameters)\n",
    "hyperparams['n_loops'] = [int(np.ceil(i * hyperparams['alpha_fraction'])) for i in hyperparams['x_dim']]\n",
    "hyperparams['x_pixels'] = np.linspace(0.5, hyperparams['image_dim'][1] - 0.5, hyperparams['image_dim'][1])\n",
    "hyperparams['y_pixels'] = np.linspace(0.5, hyperparams['image_dim'][0] - 0.5, hyperparams['image_dim'][0])\n",
    "\n",
    "encoder = CNN(n_loops_top_layer = hyperparams['n_loops'][0], x_dim_top_layer = hyperparams['x_dim'][0])\n",
    "z_sampler = sample_latents(n_loops_top_layer = hyperparams['n_loops'][0])\n",
    "readout = pen_actions_readout()\n",
    "models = (encoder, z_sampler, readout)\n",
    "\n",
    "init_params = {}\n",
    "init_params['encoder'] = encoder.init(x = np.ones((1, training_data.shape[1], training_data.shape[2])), \n",
    "                                      rngs = {'params': subkeys[0]})\n",
    "init_params['prior_z_log_var'] = np.log(0.1)\n",
    "init_params['dynamics'] = initialise_dynamics_parameters(hyperparams, subkeys[1])\n",
    "init_params['readout'] = readout.init(x = np.ones((hyperparams['x_dim'][-1])), rngs = {'params': subkeys[2]})\n",
    "init_params['pen_log_var'] = 10.0\n",
    "\n",
    "x0 = initialise_hidden_states(hyperparams)\n",
    "\n",
    "optimisation_hyperparams = {'kl_warmup_start': 500,\n",
    "                            'kl_warmup_end': 1000,\n",
    "                            'kl_min': 0.01,\n",
    "                            'kl_max': 1,\n",
    "                            'adam_b1': 0.9,\n",
    "                            'adam_b2': 0.999,\n",
    "                            'adam_eps': 1e-8,\n",
    "                            'weight_decay': 0.0001,\n",
    "                            'max_grad_norm': 10,\n",
    "                            'step_size': 0.001,\n",
    "                            'decay_steps': 1,\n",
    "                            'decay_factor': 0.9999,\n",
    "                            'batch_size': 5,\n",
    "                            'print_every': 1,\n",
    "                            'n_epochs': 10,\n",
    "                            'min_delta': 1e-3,\n",
    "                            'patience': 2}\n",
    "optimisation_hyperparams['n_batches'] = int(training_data.shape[0] / optimisation_hyperparams['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a0c17d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_scheduler(optimisation_hyperparams):\n",
    "    \n",
    "    kl_warmup_start = optimisation_hyperparams['kl_warmup_start']\n",
    "    kl_warmup_end = optimisation_hyperparams['kl_warmup_end']\n",
    "    kl_min = optimisation_hyperparams['kl_min']\n",
    "    kl_max = optimisation_hyperparams['kl_max']\n",
    "    n_batches = optimisation_hyperparams['n_batches']\n",
    "    \n",
    "    kl_schedule = []\n",
    "    for i_batch in range(n_batches):\n",
    "        \n",
    "        warm_up_fraction = min(max((i_batch - kl_warmup_start) / (kl_warmup_end - kl_warmup_start),0),1)\n",
    "        \n",
    "        kl_schedule.append(kl_min + warm_up_fraction * (kl_max - kl_min))\n",
    "\n",
    "    return np.array(kl_schedule)\n",
    "\n",
    "def reshape_training_data(training_data):\n",
    "    \n",
    "    return np.reshape(training_data, (optimisation_hyperparams['n_batches'],\n",
    "                                      optimisation_hyperparams['batch_size'],\n",
    "                                      training_data.shape[1],\n",
    "                                      training_data.shape[2]))\n",
    "\n",
    "def optimize_dynamical_VAE_core(params, x0, hyperparams, models, training_data, validation_data, optimizer, optimizer_state, \n",
    "                                optimisation_hyperparams, kl_schedule, print_every, epoch, scheduler, key):\n",
    "    \n",
    "    n_batches = optimisation_hyperparams['n_batches']\n",
    "    \n",
    "    # generate subkeys\n",
    "    key, *training_subkeys = random.split(key, num = n_batches + 1)\n",
    "    key, *validation_subkeys = random.split(key, num = int(n_batches / print_every) + 1)\n",
    "    \n",
    "    # initialise the losses and the timer\n",
    "    training_losses = {'total': 0, 'cross_entropy': 0, 'kl': 0, 'kl_prescale': 0}\n",
    "    start_time = time.time()\n",
    "\n",
    "    # loop over batches\n",
    "    for i in range(n_batches):\n",
    "        \n",
    "        i_batch = i + epoch * n_batches\n",
    "\n",
    "        kl_weight = kl_schedule[i_batch]\n",
    "        \n",
    "        (loss, all_losses), grad = training_loss_grad(params, hyperparams, models, training_data[i],\n",
    "                                                      x0, hyperparams['time_steps'], kl_weight, training_subkeys[i]) \n",
    "\n",
    "        updates, optimizer_state = optimizer.update(grad, optimizer_state, params)\n",
    "\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        \n",
    "        # training losses (average of 'print_every' batches)\n",
    "        training_losses = tree_map(lambda x, y: x + y / print_every, training_losses, all_losses)\n",
    "        \n",
    "        if (i + 1) % print_every == 0:\n",
    "        \n",
    "            # calculate loss on validation data\n",
    "            _, validation_losses = losses_jit(params, hyperparams, models, validation_data, x0, \n",
    "                                              hyperparams['time_steps'], kl_weight, validation_subkeys[int((i + 1) / print_every)])\n",
    "            \n",
    "            # end batches timer\n",
    "            batches_duration = time.time() - start_time\n",
    "\n",
    "            # print and store losses\n",
    "            s1 = '\\033[1m' + \"Batches {}-{} in {:.2f} seconds, step size: {:.5f}\" + '\\033[0m'\n",
    "            s2 = \"\"\"  Training losses {:.4f} = cross entropy {:.4f} + KL {:.4f} ({:.4f})\"\"\"\n",
    "            s3 = \"\"\"  Validation losses {:.4f} = cross entropy {:.4f} + KL {:.4f} ({:.4f})\"\"\"\n",
    "            print(s1.format(i + 1 - print_every + 1, i + 1, batches_duration, scheduler(i_batch)))\n",
    "            print(s2.format(training_losses['total'], training_losses['cross_entropy'],\n",
    "                            training_losses['kl'], training_losses['kl_prescale']))\n",
    "            print(s3.format(validation_losses['total'], validation_losses['cross_entropy'],\n",
    "                            validation_losses['kl'], validation_losses['kl_prescale']))\n",
    "\n",
    "            if (i + 1) == print_every:\n",
    "                tlosses_thru_training = copy(training_losses)\n",
    "                vlosses_thru_training = copy(validation_losses)\n",
    "            else:\n",
    "                tlosses_thru_training = tree_map(lambda x, y: np.append(x, y), tlosses_thru_training, training_losses)\n",
    "                vlosses_thru_training = tree_map(lambda x, y: np.append(x, y), vlosses_thru_training, validation_losses)\n",
    "            \n",
    "            # re-initialise the losses and timer\n",
    "            training_losses = {'total': 0, 'cross_entropy': 0, 'kl': 0, 'kl_prescale': 0}\n",
    "            start_time = time.time()\n",
    "            \n",
    "    losses = {'tlosses' : tlosses_thru_training, 'vlosses' : vlosses_thru_training}\n",
    "\n",
    "    return params, optimizer_state, losses\n",
    "\n",
    "def optimize_dynamical_VAE(params, x0, hyperparams, models, training_data, validation_data, \n",
    "                           optimisation_hyperparams, key, ckpt_dir, writer):\n",
    "\n",
    "    kl_schedule = kl_scheduler(optimisation_hyperparams)\n",
    "    \n",
    "    scheduler = optax.exponential_decay(optimisation_hyperparams['step_size'], \n",
    "                                        optimisation_hyperparams['decay_steps'], \n",
    "                                        optimisation_hyperparams['decay_factor'])\n",
    "\n",
    "    optimizer = optax.chain(optax.adamw(learning_rate = scheduler, \n",
    "                            b1 = optimisation_hyperparams['adam_b1'],\n",
    "                            b2 = optimisation_hyperparams['adam_b2'],\n",
    "                            eps = optimisation_hyperparams['adam_eps'],\n",
    "                            weight_decay = optimisation_hyperparams['weight_decay']),\n",
    "                            optax.clip_by_global_norm(optimisation_hyperparams['max_grad_norm']))\n",
    "    \n",
    "    optimizer_state = optimizer.init(params)\n",
    "    \n",
    "    # reshape training data\n",
    "    training_data = reshape_training_data(training_data)\n",
    "    \n",
    "    # set early stopping criteria\n",
    "    early_stop = EarlyStopping(min_delta = optimisation_hyperparams['min_delta'], \n",
    "                               patience = optimisation_hyperparams['patience'])\n",
    "    \n",
    "    # loop over epochs\n",
    "    n_epochs = optimisation_hyperparams['n_epochs']\n",
    "    print_every = optimisation_hyperparams['print_every']\n",
    "    losses = {}\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # start epoch timer\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # generate subkeys\n",
    "        key, *subkeys = random.split(key, num = 3)\n",
    "        \n",
    "        # shuffle the batches every epoch\n",
    "        data = random.permutation(subkeys[0], training_data, axis = 0)\n",
    "        \n",
    "        # perform optimisation\n",
    "        params, optimizer_state, losses['epoch ' + str(epoch)] = \\\n",
    "        optimize_dynamical_VAE_core(params, x0, hyperparams, models, data, validation_data, optimizer, optimizer_state, \n",
    "                                    optimisation_hyperparams, kl_schedule, print_every, epoch, scheduler, subkeys[1])\n",
    "        \n",
    "        # end epoch timer\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        \n",
    "        # print metrics (mean over all batches in epoch) \n",
    "        s1 = '\\033[1m' + \"Epoch {} in {:.1f} minutes\" + '\\033[0m'\n",
    "        s2 = \"\"\"  Training losses {:.4f} = cross entropy {:.4f} + KL {:.4f} ({:.4f})\"\"\"\n",
    "        s3 = \"\"\"  Validation losses {:.4f} = cross entropy {:.4f} + KL {:.4f}, ({:.4f})\\n\"\"\"\n",
    "        print(s1.format(epoch + 1, epoch_duration / 60))\n",
    "        tlosses = losses['epoch ' + str(epoch)]['tlosses']\n",
    "        print(s2.format(tlosses['total'].mean(), tlosses['cross_entropy'].mean(),\n",
    "                        tlosses['kl'].mean(), tlosses['kl_prescale'].mean()))\n",
    "        vlosses = losses['epoch ' + str(epoch)]['vlosses']\n",
    "        print(s3.format(vlosses['total'].mean(), vlosses['cross_entropy'].mean(),\n",
    "                        vlosses['kl'].mean(), vlosses['kl_prescale'].mean()))\n",
    "\n",
    "        # write metrics to tensorboard\n",
    "        writer.scalar('loss (train)', tlosses['total'].mean(), epoch)\n",
    "        writer.scalar('cross entropy (train)', tlosses['cross_entropy'].mean(), epoch)\n",
    "        writer.scalar('KL (train)', tlosses['kl'].mean(), epoch)\n",
    "        writer.scalar('KL prescale (train)', tlosses['kl_prescale'].mean(), epoch)\n",
    "        writer.scalar('loss (validation)', vlosses['total'].mean(), epoch)\n",
    "        writer.scalar('cross entropy (validation)', vlosses['cross_entropy'].mean(), epoch)\n",
    "        writer.scalar('KL (validation)', vlosses['kl'].mean(), epoch)\n",
    "        writer.scalar('KL prescale (validation)', vlosses['kl_prescale'].mean(), epoch)\n",
    "        writer.flush()\n",
    "        \n",
    "        # save checkpoint\n",
    "        ckpt = {'params': params, 'optimizer_state': optimizer_state, 'losses': losses}\n",
    "        checkpoints.save_checkpoint(ckpt_dir = ckpt_dir, target = ckpt, step = epoch)\n",
    "        \n",
    "        # if early stopping criteria met, break\n",
    "        _, early_stop = early_stop.update(vlosses['total'].mean())\n",
    "        if early_stop.should_stop:\n",
    "            \n",
    "            print('Early stopping criteria met, breaking...')\n",
    "            \n",
    "            break\n",
    "            \n",
    "    return params, optimizer_state, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aedcd70-655c-4fa2-9b32-fca3236ded0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching TensorBoard...\n",
      "Please visit http://localhost:6006 in a web browser.\n",
      "\u001b[1mBatches 1-1 in 20.76 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.4011 = cross entropy 0.6644 + KL 0.7367 (73.6715)\n",
      "  Validation losses 1.4568 = cross entropy 0.7225 + KL 0.7344 (73.4391)\n",
      "\u001b[1mBatches 2-2 in 15.77 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.4255 = cross entropy 0.6908 + KL 0.7347 (73.4739)\n",
      "  Validation losses 1.4666 = cross entropy 0.7342 + KL 0.7324 (73.2424)\n",
      "\u001b[1mBatches 3-3 in 7.78 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.4159 = cross entropy 0.6832 + KL 0.7328 (73.2768)\n",
      "  Validation losses 1.4048 = cross entropy 0.6744 + KL 0.7305 (73.0463)\n",
      "\u001b[1mBatches 4-4 in 7.53 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.4306 = cross entropy 0.6998 + KL 0.7308 (73.0804)\n",
      "  Validation losses 1.4584 = cross entropy 0.7299 + KL 0.7285 (72.8508)\n",
      "\u001b[1mBatches 5-5 in 7.19 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.4384 = cross entropy 0.7096 + KL 0.7288 (72.8847)\n",
      "  Validation losses 1.3927 = cross entropy 0.6661 + KL 0.7266 (72.6562)\n",
      "\u001b[1mBatches 6-6 in 6.69 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3986 = cross entropy 0.6717 + KL 0.7269 (72.6898)\n",
      "  Validation losses 1.4246 = cross entropy 0.7000 + KL 0.7246 (72.4622)\n",
      "\u001b[1mBatches 7-7 in 5.89 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3842 = cross entropy 0.6592 + KL 0.7250 (72.4955)\n",
      "  Validation losses 1.4306 = cross entropy 0.7079 + KL 0.7227 (72.2689)\n",
      "\u001b[1mBatches 8-8 in 6.85 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3828 = cross entropy 0.6598 + KL 0.7230 (72.3019)\n",
      "  Validation losses 1.4211 = cross entropy 0.7003 + KL 0.7208 (72.0759)\n",
      "\u001b[1mBatches 9-9 in 6.71 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.4191 = cross entropy 0.6980 + KL 0.7211 (72.1088)\n",
      "  Validation losses 1.4027 = cross entropy 0.6839 + KL 0.7188 (71.8836)\n",
      "\u001b[1mBatches 10-10 in 7.55 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.5019 = cross entropy 0.7828 + KL 0.7192 (71.9162)\n",
      "  Validation losses 1.4022 = cross entropy 0.6853 + KL 0.7169 (71.6917)\n",
      "\u001b[1mBatches 11-11 in 6.29 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.4121 = cross entropy 0.6949 + KL 0.7172 (71.7241)\n",
      "  Validation losses 1.3884 = cross entropy 0.6734 + KL 0.7150 (71.5004)\n",
      "\u001b[1mBatches 12-12 in 5.84 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3793 = cross entropy 0.6640 + KL 0.7153 (71.5325)\n",
      "  Validation losses 1.3753 = cross entropy 0.6622 + KL 0.7131 (71.3096)\n",
      "\u001b[1mBatches 13-13 in 5.85 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3721 = cross entropy 0.6587 + KL 0.7134 (71.3417)\n",
      "  Validation losses 1.3780 = cross entropy 0.6668 + KL 0.7112 (71.1196)\n",
      "\u001b[1mBatches 14-14 in 6.72 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3835 = cross entropy 0.6719 + KL 0.7115 (71.1514)\n",
      "  Validation losses 1.3819 = cross entropy 0.6726 + KL 0.7093 (70.9300)\n",
      "\u001b[1mBatches 15-15 in 6.03 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3729 = cross entropy 0.6633 + KL 0.7096 (70.9617)\n",
      "  Validation losses 1.3669 = cross entropy 0.6594 + KL 0.7074 (70.7411)\n",
      "\u001b[1mBatches 16-16 in 5.09 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3788 = cross entropy 0.6710 + KL 0.7077 (70.7726)\n",
      "  Validation losses 1.3627 = cross entropy 0.6572 + KL 0.7055 (70.5527)\n",
      "\u001b[1mBatches 17-17 in 5.17 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3721 = cross entropy 0.6663 + KL 0.7058 (70.5840)\n",
      "  Validation losses 1.3612 = cross entropy 0.6576 + KL 0.7036 (70.3649)\n",
      "\u001b[1mBatches 18-18 in 5.82 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3610 = cross entropy 0.6570 + KL 0.7040 (70.3961)\n",
      "  Validation losses 1.3584 = cross entropy 0.6566 + KL 0.7018 (70.1777)\n",
      "\u001b[1mBatches 19-19 in 4.99 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3718 = cross entropy 0.6698 + KL 0.7021 (70.2086)\n",
      "  Validation losses 1.3555 = cross entropy 0.6556 + KL 0.6999 (69.9909)\n",
      "\u001b[1mBatches 20-20 in 6.30 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3650 = cross entropy 0.6648 + KL 0.7002 (70.0217)\n",
      "  Validation losses 1.3570 = cross entropy 0.6590 + KL 0.6980 (69.8045)\n",
      "\u001b[1mBatches 21-21 in 6.14 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3534 = cross entropy 0.6550 + KL 0.6984 (69.8353)\n",
      "  Validation losses 1.3491 = cross entropy 0.6529 + KL 0.6962 (69.6188)\n",
      "\u001b[1mBatches 22-22 in 6.69 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3496 = cross entropy 0.6531 + KL 0.6965 (69.6494)\n",
      "  Validation losses 1.3466 = cross entropy 0.6523 + KL 0.6943 (69.4335)\n",
      "\u001b[1mBatches 23-23 in 5.68 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3466 = cross entropy 0.6520 + KL 0.6946 (69.4641)\n",
      "  Validation losses 1.3488 = cross entropy 0.6564 + KL 0.6925 (69.2488)\n",
      "\u001b[1mBatches 24-24 in 4.65 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3450 = cross entropy 0.6522 + KL 0.6928 (69.2793)\n",
      "  Validation losses 1.3453 = cross entropy 0.6546 + KL 0.6906 (69.0646)\n",
      "\u001b[1mBatches 25-25 in 5.51 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3443 = cross entropy 0.6534 + KL 0.6910 (69.0950)\n",
      "  Validation losses 1.3403 = cross entropy 0.6515 + KL 0.6888 (68.8810)\n",
      "\u001b[1mBatches 26-26 in 5.84 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3400 = cross entropy 0.6509 + KL 0.6891 (68.9112)\n",
      "  Validation losses 1.3420 = cross entropy 0.6550 + KL 0.6870 (68.6979)\n",
      "\u001b[1mBatches 27-27 in 5.18 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3396 = cross entropy 0.6523 + KL 0.6873 (68.7280)\n",
      "  Validation losses 1.3369 = cross entropy 0.6518 + KL 0.6852 (68.5152)\n",
      "\u001b[1mBatches 28-28 in 5.72 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3350 = cross entropy 0.6495 + KL 0.6855 (68.5453)\n",
      "  Validation losses 1.3350 = cross entropy 0.6517 + KL 0.6833 (68.3331)\n",
      "\u001b[1mBatches 29-29 in 5.87 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3324 = cross entropy 0.6488 + KL 0.6836 (68.3631)\n",
      "  Validation losses 1.3310 = cross entropy 0.6495 + KL 0.6815 (68.1515)\n",
      "\u001b[1mBatches 30-30 in 5.83 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3297 = cross entropy 0.6478 + KL 0.6818 (68.1814)\n",
      "  Validation losses 1.3292 = cross entropy 0.6495 + KL 0.6797 (67.9705)\n",
      "\u001b[1mBatches 31-31 in 5.29 seconds, step size: 0.00100\u001b[0m\n",
      "  Training losses 1.3288 = cross entropy 0.6488 + KL 0.6800 (68.0003)\n",
      "  Validation losses 1.3282 = cross entropy 0.6503 + KL 0.6779 (67.7899)\n"
     ]
    }
   ],
   "source": [
    "from jax.config import config\n",
    "config.update(\"jax_debug_nans\", True)\n",
    "config.update(\"jax_disable_jit\", False)\n",
    "# use xeus-python kernel -- Python 3.9 (XPython) -- for debugging\n",
    "# typing help at a breakpoint() gives you list of available commands\n",
    "\n",
    "from flax.metrics import tensorboard\n",
    "log_folder = \"runs/exp9/profile\"\n",
    "writer = tensorboard.SummaryWriter(log_folder)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs/exp9\n",
    "\n",
    "ckpt_dir = 'tmp/flax-checkpointing'\n",
    "\n",
    "import shutil\n",
    "if os.path.exists(ckpt_dir):\n",
    "    shutil.rmtree(ckpt_dir) # remove any existing checkpoints from the last notebook run\n",
    "\n",
    "trained_params, optimizer_state, losses = \\\n",
    "optimize_dynamical_VAE(init_params, x0, hyperparams, models, training_data, validation_data, \n",
    "                       optimisation_hyperparams, key, ckpt_dir, writer)\n",
    "\n",
    "# # restore checkpoint\n",
    "# ckpt = {'params': trained_params, 'optimizer_state': optimizer_state, 'losses': losses}\n",
    "# restored_state = checkpoints.restore_checkpoint(ckpt_dir = ckpt_dir, target = ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ab104-b63e-4763-aa58-63bdc59e73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run model and plot\n",
    "# T = 100\n",
    "# data = training_data[0,:,:]\n",
    "# hyperparams['dt'] = 0.01\n",
    "# hyperparams['time_steps'] = 1000\n",
    "\n",
    "# params = init_params\n",
    "# results_encode = encode(models, params, data[None,:,:])\n",
    "# _, z_sampler, _ = models\n",
    "# z1, z2 = z_sampler(results_encode, params, hyperparams, key)\n",
    "# A = construct_dynamics_matrix(params, hyperparams)\n",
    "# results_decode = batch_decode(params, hyperparams, models, A, x0, T, z1, z2)\n",
    "# results = results_encode | results_decode\n",
    "\n",
    "# for ex in range(1):\n",
    "#     plt.scatter(results['pen_xy'][ex][:,0],results['pen_xy'][ex][:,1], c = 'k', alpha =  np.exp(results['pen_down_log_p'][ex,:]))\n",
    "# plt.ylim(0,105)\n",
    "# plt.xlim(0,105)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd497aae-5366-4e41-bc58-27188c6c6d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of time when running model forward is decoder and x-entropy functions\n",
    "\n",
    "# replace cnn with capsule?\n",
    "# do i need to standardize images (and how) or is normalised (as is) fine?\n",
    "# ultimately replace pen with myosuite finger to make it a finger painting task - could have flexion/extension determine finger up/down\n",
    "\n",
    "# pen_xy0 options: always start in center of canvas (105/2, 105/2), randomise and use extra feature dimension in image, let the CNN choose\n",
    "# consider learning initial neural states\n",
    "# add control costs? e.g. squared velocity costs?\n",
    "# add batch norm and other tricks to CNN, dropout?\n",
    "\n",
    "# if there are l layers (e.g. 3), the top-layer alphas on the last l-1 time steps (e.g. 2) don't influence the state of the lowest layer and hence the objective\n",
    "# when n_layer = 3, top-layer alphas at time 1 inlfuence the state of the lowest layer and hence the objective at time 3, and so on\n",
    "# this leads to zeros in the biases and columns of weight matrix in last dense layer\n",
    "\n",
    "# relu activation function, and to some extent binary image data, can lead to zero gradients scattered throughout CNN\n",
    "# these gradients go to zero if you change relu to tanh and make data continuous on [0, 1], so not pathological, i don't think\n",
    "\n",
    "# print values without tracer information\n",
    "# jax.debug.print(\"{z_mean}\", z_mean = z_mean)\n",
    "# jax.debug.breakpoint() - didn't work for me"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
