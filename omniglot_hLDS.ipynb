{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57469386-cfb5-4f1c-a58b-5b9df4965138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random, lax, jit, vmap, value_and_grad\n",
    "from jax.tree_util import tree_map\n",
    "from jax.nn import initializers\n",
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "from flax import linen as nn\n",
    "from flax.core.frozen_dict import freeze, unfreeze\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], device_type = 'GPU')\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import optax\n",
    "from flax.training import checkpoints, train_state\n",
    "from flax.training.early_stopping import EarlyStopping\n",
    "\n",
    "import time\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "658e9930-284e-4f27-8f32-6f4f67679b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    n_loops_top_layer: int\n",
    "    x_dim_top_layer: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # CNN based on End-to-End Training of Deep Visuomotor Policies\n",
    "        x = nn.Conv(features = 64, kernel_size = (7, 7))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features = 32, kernel_size = (5, 5))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features = 32, kernel_size = (5, 5))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = x.reshape((x.shape[0], -1)) # flatten\n",
    "        x = nn.Dense(features = 64)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features = 40)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features = 40)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features = (self.n_loops_top_layer + self.x_dim_top_layer) * 2)(x)\n",
    "        \n",
    "        # mean and log variances of Gaussian distribution over latents\n",
    "        z_mean, z_log_var = np.split(x, 2, axis = 1)\n",
    "        \n",
    "        return {'z_mean': z_mean, \n",
    "                'z_log_var': z_log_var}\n",
    "    \n",
    "class sampler(nn.Module):\n",
    "    n_loops_top_layer: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, results, params, hyperparams, key):\n",
    "        \n",
    "        def diag_Gaussian_sample(mean, log_var, hyperparams, key):\n",
    "            \"\"\"\n",
    "            sample from a diagonal Gaussian distribution\n",
    "            \"\"\"\n",
    "            log_var = stabilise_varance(log_var, hyperparams)\n",
    "\n",
    "            return mean + np.exp(0.5 * log_var) * random.normal(key, mean.shape)\n",
    "\n",
    "        # sample the latents\n",
    "        z = diag_Gaussian_sample(results['z_mean'], results['z_log_var'], hyperparams, key)\n",
    "\n",
    "        # split the latents into top-layer alphas, softmax(z1), and initial state, z2\n",
    "        z1, z2 = np.split(z, [self.n_loops_top_layer], axis = 1)\n",
    "\n",
    "        return nn.activation.softmax(z1 / params['t'][0], axis = 1), np.squeeze(z2)\n",
    "\n",
    "class decoder(nn.Module):\n",
    "    T: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, params, A, hyperparams, x0, z1, z2):        \n",
    "\n",
    "        def decode_one_step(carry, inputs):\n",
    "            \n",
    "            def compute_alphas(W, x, b, t):\n",
    "\n",
    "                return nn.activation.softmax( (W @ x + b) / t, axis = 0)\n",
    "\n",
    "            def compute_inputs(W, x):\n",
    "\n",
    "                return W @ x\n",
    "\n",
    "            def update_state(A, x, alphas, u, dt):\n",
    "\n",
    "                return x + (np.sum(alphas[:, None, None] * A, axis = 0) @ x + u) * dt\n",
    "\n",
    "            def compute_pen_actions(W, x, b):\n",
    "\n",
    "                return W @ x + b\n",
    "            \n",
    "            def per_pixel_bernoulli_parameter(params, hyperparams, pen_xy, pen_down_log_p):\n",
    "    \n",
    "                def log_Gaussian_kernel(x, mu, log_var, hyperparams):\n",
    "                    \"\"\"\n",
    "                    calculate the log likelihood of x under a diagonal Gaussian distribution\n",
    "                    \"\"\"\n",
    "                    log_var = stabilise_varance(log_var, hyperparams)\n",
    "\n",
    "                    return -0.5 * (x - mu)**2 / np.exp(log_var)\n",
    "\n",
    "                ll_p_x = log_Gaussian_kernel(pen_xy[0], hyperparams['x_pixels'], params['pen_log_var'], hyperparams)\n",
    "                ll_p_y = log_Gaussian_kernel(pen_xy[1], hyperparams['y_pixels'], params['pen_log_var'], hyperparams)\n",
    "\n",
    "                p_xy_t = np.exp(ll_p_x[None,:] + ll_p_y[:,None] + pen_down_log_p)\n",
    "\n",
    "                return p_xy_t\n",
    "            \n",
    "            def update_pen_position(pen_xy, d_xy, hyperparams):\n",
    "    \n",
    "                # candidate new pen position\n",
    "                pen_xy = pen_xy + d_xy\n",
    "\n",
    "                # align pen position relative to centre of canvas\n",
    "                pen_xy = pen_xy - hyperparams['image_dim'] / 2\n",
    "\n",
    "                # transform canvas boundaries to -/+ 5\n",
    "                pen_xy = pen_xy * 2 / hyperparams['image_dim'] * 5\n",
    "\n",
    "                # squash pen position to be within canvas boundaries\n",
    "                pen_xy = nn.sigmoid(pen_xy)\n",
    "\n",
    "                # transform canvas boundaries back to their original values\n",
    "                pen_xy_new = pen_xy * hyperparams['image_dim']\n",
    "\n",
    "                return pen_xy_new\n",
    "\n",
    "            x, pen_xy = carry\n",
    "            top_layer_alphas = inputs\n",
    "\n",
    "            # compute the alphas\n",
    "            alphas = jax.tree_map(compute_alphas, params['W_a'], x[:2], params['b_a'], params['t'][1:])\n",
    "\n",
    "            # prepend the top-layer alphas\n",
    "            alphas.insert(0, np.squeeze(top_layer_alphas))\n",
    "\n",
    "            # compute the additive inputs\n",
    "            u = jax.tree_map(compute_inputs, params['W_u'], x[:2])\n",
    "\n",
    "            # prepend the top-layer additive inputs\n",
    "            u.insert(0, x[0] * 0)\n",
    "\n",
    "            # update the states\n",
    "            x_new = jax.tree_map(update_state, A, x, alphas, u, hyperparams['dt'])\n",
    "\n",
    "            # linear readout from the state at the bottom layer\n",
    "            pen_actions = compute_pen_actions(params['W_p'], x_new[-1], params['b_p'])\n",
    "\n",
    "            # pen velocities in x and y directions\n",
    "            d_xy = pen_actions[:2]\n",
    "\n",
    "            # log probability that the pen is down (actively drawing)\n",
    "            pen_down_log_p = nn.log_sigmoid(pen_actions[2])\n",
    "\n",
    "            # calculate the per-pixel bernoulli parameter\n",
    "            p_xy = per_pixel_bernoulli_parameter(params, hyperparams, pen_xy, pen_down_log_p)\n",
    "\n",
    "            # update the pen position based on the pen velocity\n",
    "            pen_xy_new = update_pen_position(pen_xy, d_xy, hyperparams)\n",
    "\n",
    "            carry = x_new, pen_xy_new\n",
    "            outputs = alphas, x_new, pen_xy_new, p_xy, pen_down_log_p\n",
    "\n",
    "            return carry, outputs\n",
    "\n",
    "        x0[0] = z2[:]\n",
    "\n",
    "        pen_xy0 = hyperparams['image_dim'] / 2 # initialise pen in centre of canvas\n",
    "\n",
    "        carry = x0, pen_xy0\n",
    "        inputs = np.repeat(z1[None,:], self.T, axis = 0)\n",
    "\n",
    "        _, (alphas, x, pen_xy, p_xy_t, pen_down_log_p) = lax.scan(decode_one_step, carry, inputs)\n",
    "    \n",
    "        return {'alphas': alphas,\n",
    "                'x0': x0,\n",
    "                'x': x,\n",
    "                'pen_xy0': pen_xy0,\n",
    "                'pen_xy': pen_xy,\n",
    "                'p_xy_t': p_xy_t,\n",
    "                'pen_down_log_p': pen_down_log_p}\n",
    "\n",
    "def stabilise_varance(log_var, hyperparams):\n",
    "    \"\"\"\n",
    "    var_min is added to the variances for numerical stability\n",
    "    \"\"\"\n",
    "    return np.log(np.exp(log_var) + hyperparams['var_min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ab7456e-7d80-4ef6-87aa-2536023c4cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    n_loops_top_layer: int\n",
    "    x_dim_top_layer: int\n",
    "    T: int\n",
    "\n",
    "    def setup(self):\n",
    "        \n",
    "        self.encoder = encoder(self.n_loops_top_layer, self.x_dim_top_layer)\n",
    "        self.sampler = sampler(self.n_loops_top_layer)\n",
    "        self.decoder = decoder(self.T)\n",
    "\n",
    "    def __call__(self, data, params, hyperparams, key, A, x0):\n",
    "        \n",
    "        results_encode = self.encoder(data[None,:,:,None])\n",
    "        z1, z2 = self.sampler(results_encode, params, hyperparams, key)\n",
    "        results_decode = self.decoder(params, A, hyperparams, x0, z1, z2)\n",
    "        \n",
    "        return results_encode | results_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43d39ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_decoder_parameters(hyperparams,  key):\n",
    "    \n",
    "    P = []\n",
    "    S_U = []\n",
    "    S_V = []\n",
    "    L = []\n",
    "    W_u = []\n",
    "    W_a = []\n",
    "    b_a = []\n",
    "    t = []\n",
    "    \n",
    "    n_layers = len(hyperparams['x_dim'])\n",
    "    for layer in range(n_layers):\n",
    "        \n",
    "        key, *subkeys = random.split(key, num = 8)\n",
    "        \n",
    "        n_loops = hyperparams['n_loops'][layer]\n",
    "        x_dim = hyperparams['x_dim'][layer]\n",
    "        \n",
    "        # parameters of layer-specific P\n",
    "        p = random.normal(subkeys[0], (x_dim, x_dim))\n",
    "        \n",
    "        # set trace of P @ P.T to x_dim\n",
    "        P.append(p * np.sqrt(x_dim / np.trace(p @ p.T)))\n",
    "        \n",
    "        # parameters of layer- and loop-specific S\n",
    "        u = random.normal(subkeys[1], (n_loops, x_dim, int(x_dim / n_loops)))\n",
    "        v = random.normal(subkeys[2], (n_loops, x_dim, int(x_dim / n_loops)))\n",
    "        \n",
    "        # set variance of elements of S to 1/(n_loops * x_dim)\n",
    "        s = u @ np.transpose(v, (0, 2, 1)) - v @ np.transpose(u, (0, 2, 1))\n",
    "        # f = 1 / np.linalg.norm(s, axis = (1, 2))[:, None, None] / n_loops # frobenius norm of each loop 1/n_loops\n",
    "        f = 1 / np.std(s, axis = (1, 2))[:, None, None] / np.sqrt(n_loops * x_dim)\n",
    "        S_U.append(u * np.sqrt(f))\n",
    "        S_V.append(v * np.sqrt(f))\n",
    "\n",
    "        # parameters of layer- and loop-specific L\n",
    "        Q, _ = np.linalg.qr(random.normal(subkeys[3], (x_dim, x_dim)))\n",
    "        L_i = np.split(Q * np.sqrt(n_loops), n_loops, axis = 1)\n",
    "        L.append(np.stack(L_i, axis = 0))\n",
    "\n",
    "        # parameters of the mapping from hidden states to alphas, alphas = softmax(W @ x + b, temperature)\n",
    "        if layer != 0:\n",
    "            \n",
    "            # weights for additive inputs\n",
    "            std_W = 1 / np.sqrt(hyperparams['x_dim'][layer - 1])\n",
    "            W_u.append(random.normal(subkeys[4], (hyperparams['x_dim'][layer], hyperparams['x_dim'][layer - 1])) * std_W)\n",
    "            \n",
    "            # weights for modulatory factors\n",
    "            W_a.append(random.normal(subkeys[5], (n_loops, hyperparams['x_dim'][layer - 1])) * std_W)\n",
    "\n",
    "            # bias for modulatory factors\n",
    "            b_a.append(np.zeros((n_loops)))\n",
    "            \n",
    "        if layer == n_layers - 1:\n",
    "            \n",
    "            # weights for pen actions\n",
    "            std_W = 1 / np.sqrt(hyperparams['x_dim'][layer])\n",
    "            W_p = random.normal(subkeys[6], (3, hyperparams['x_dim'][layer])) * std_W\n",
    "            \n",
    "            # bias for pen actions\n",
    "            b_p = np.zeros((3))\n",
    "            \n",
    "        # temperature of layer-specific softmax function\n",
    "        t.append(1.0)\n",
    "\n",
    "    return {'P': P, \n",
    "            'S_U': S_U,\n",
    "            'S_V': S_V, \n",
    "            'L': L, \n",
    "            'W_u': W_u,\n",
    "            'W_a': W_a, \n",
    "            'b_a': b_a,\n",
    "            't': t,\n",
    "            'W_p': W_p,\n",
    "            'b_p': b_p,\n",
    "            'pen_log_var': hyperparams['init_pen_log_var']}\n",
    "\n",
    "def construct_dynamics_matrix(params, hyperparams):\n",
    "\n",
    "    def construct_P(P):\n",
    "        return P @ P.T\n",
    "\n",
    "    def construct_S(U, V):\n",
    "        return U @ np.transpose(V, (0, 2, 1)) - V @ np.transpose(U, (0, 2, 1))\n",
    "\n",
    "    def construct_A(L, P, S):\n",
    "        return (-L @ np.transpose(L, (0, 2, 1)) + S) @ P\n",
    "\n",
    "    # positive semi-definite matrix P\n",
    "    P = jax.tree_map(construct_P, params['P'])\n",
    "\n",
    "    # skew symmetric matrix S\n",
    "    S = jax.tree_map(construct_S, params['S_U'], params['S_V'])\n",
    "\n",
    "    # dynamics matrix A (loops organised along axis 0)\n",
    "    A = jax.tree_map(construct_A, params['L'], S, P)\n",
    "\n",
    "    return A\n",
    "\n",
    "def initialise_LDS_states(hyperparams):\n",
    "              \n",
    "    # initialise the states of the LDS in the decoder to zero (not learned)\n",
    "    # the state of the top layer will be inferred later by the encoder and so the value here will be overwritten\n",
    "    n_layers = len(hyperparams['x_dim'])\n",
    "    init_states = []\n",
    "    for layer in range(n_layers):\n",
    "        \n",
    "        init_states.append(np.zeros(hyperparams['x_dim'][layer]))\n",
    "\n",
    "    return init_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43c38ce5-00ff-4c77-8d28-d842e423d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://jax.readthedocs.io/en/latest/notebooks/neural_network_with_tfds_data.html\n",
    "# https://www.tensorflow.org/datasets/catalog/omniglot\n",
    "# https://www.tensorflow.org/datasets/api_docs/python/tfds/load\n",
    "\n",
    "(full_train_set, test_dataset), ds_info = \\\n",
    "tfds.load('Omniglot', split = ['train', 'test'], shuffle_files = True, as_supervised = False, with_info = True)\n",
    "\n",
    "def prepare_image(dictionary):\n",
    "    \n",
    "    # remove redundant channels\n",
    "    dictionary['image'] = dictionary['image'][:,:,0]\n",
    "    \n",
    "    # invert image so drawn pixels are 1\n",
    "    dictionary['image'] = tf.cast(dictionary['image'] == 0, tf.float32)\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "def transform_dataset(dataset, batch_size, tfds_seed):\n",
    "    \n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(tf.data.experimental.cardinality(dataset).numpy(), seed = tfds_seed, reshuffle_each_iteration = True)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder = True).prefetch(1)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "full_train_set = full_train_set.map(prepare_image, num_parallel_calls = tf.data.AUTOTUNE)\n",
    "\n",
    "validation_split = 0.2\n",
    "num_data = tf.data.experimental.cardinality(full_train_set).numpy()\n",
    "train_dataset = full_train_set.take(num_data * (1 - validation_split))\n",
    "val_dataset = full_train_set.take(num_data * (validation_split))\n",
    "\n",
    "tfds_seed = 0\n",
    "batch_size = 5\n",
    "train_dataset = transform_dataset(train_dataset, batch_size, tfds_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fca12b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary hyperparameters\n",
    "hyperparams = {'jax_seed': 0,\n",
    "               'tfds_seed': tfds_seed,\n",
    "               'x_dim': [20, 50, 200],\n",
    "               'alpha_fraction': 0.1,\n",
    "               'dt': [0.01, 0.01, 0.01],\n",
    "               'time_steps': 100,\n",
    "               'var_min': 1e-16,\n",
    "               'smooth_max_parameter': 1e3,\n",
    "               'init_pen_log_var': 10.0,\n",
    "               'image_dim': np.array(ds_info.features['image'].shape[:2])}\n",
    "\n",
    "# secondary hyperparameters (derived from primary hyperparameters)\n",
    "hyperparams['n_loops'] = [int(np.ceil(i * hyperparams['alpha_fraction'])) for i in hyperparams['x_dim']]\n",
    "hyperparams['x_pixels'] = np.linspace(0.5, hyperparams['image_dim'][1] - 0.5, hyperparams['image_dim'][1])\n",
    "hyperparams['y_pixels'] = np.linspace(0.5, hyperparams['image_dim'][0] - 0.5, hyperparams['image_dim'][0])\n",
    "\n",
    "optimisation_hyperparams = {'kl_warmup_start': 500,\n",
    "                            'kl_warmup_end': 1000,\n",
    "                            'kl_min': 0.01,\n",
    "                            'kl_max': 1,\n",
    "                            'adam_b1': 0.9,\n",
    "                            'adam_b2': 0.999,\n",
    "                            'adam_eps': 1e-8,\n",
    "                            'weight_decay': 0.0001,\n",
    "                            'max_grad_norm': 10,\n",
    "                            'step_size': 0.001,\n",
    "                            'decay_steps': 1,\n",
    "                            'decay_factor': 0.9999,\n",
    "                            'batch_size': batch_size,\n",
    "                            'print_every': 1,\n",
    "                            'n_epochs': 10,\n",
    "                            'n_batches': len(train_dataset),\n",
    "                            'min_delta': 1e-3,\n",
    "                            'patience': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "266735bb-4ed1-4e0c-a5f5-ff7e4cf83c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly generate a PRNG key\n",
    "key = random.PRNGKey(hyperparams['jax_seed'])\n",
    "\n",
    "# generate the required number of subkeys\n",
    "key, *subkeys = random.split(key, num = 3)\n",
    "\n",
    "# initialise model parameters and LDS states\n",
    "x0 = initialise_LDS_states(hyperparams)\n",
    "model = VAE(n_loops_top_layer = hyperparams['n_loops'][0], x_dim_top_layer = hyperparams['x_dim'][0], T = hyperparams['time_steps'])\n",
    "params = {}\n",
    "params['prior_z_log_var'] = np.log(0.1)\n",
    "params['decoder'] = initialise_decoder_parameters(hyperparams, subkeys[0])\n",
    "init_params = model.init(data = np.ones((1, hyperparams['image_dim'][0], hyperparams['image_dim'][1], 1)), \n",
    "                         params = params['decoder'], hyperparams = hyperparams, key = subkeys[1], \n",
    "                         A = construct_dynamics_matrix(params['decoder'], hyperparams), x0 = x0, \n",
    "                         rngs = {'params': random.PRNGKey(0)})['params']\n",
    "\n",
    "# concatenate all params into one dictionary\n",
    "init_params = unfreeze(init_params)\n",
    "init_params = init_params | params\n",
    "init_params = freeze(init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd13f143-3753-4ec0-8a84-6dd2271c4dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run the model\n",
    "# A = construct_dynamics_matrix(init_params, hyperparams)\n",
    "# results = model.apply({'params': init_params}, training_data[0,:,:], params, hyperparams, key, A, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec7a9a4b-c745-41b2-a640-b7ee09f37517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_scheduler(optimisation_hyperparams):\n",
    "    \n",
    "    kl_warmup_start = optimisation_hyperparams['kl_warmup_start']\n",
    "    kl_warmup_end = optimisation_hyperparams['kl_warmup_end']\n",
    "    kl_min = optimisation_hyperparams['kl_min']\n",
    "    kl_max = optimisation_hyperparams['kl_max']\n",
    "    n_batches = optimisation_hyperparams['n_batches']\n",
    "    \n",
    "    kl_schedule = []\n",
    "    for i_batch in range(n_batches):\n",
    "        \n",
    "        warm_up_fraction = min(max((i_batch - kl_warmup_start) / (kl_warmup_end - kl_warmup_start), 0), 1)\n",
    "        \n",
    "        kl_schedule.append(kl_min + warm_up_fraction * (kl_max - kl_min))\n",
    "\n",
    "    return iter(kl_schedule)\n",
    "\n",
    "def create_train_state(model, params, optimisation_hyperparams):\n",
    "    \n",
    "    lr_scheduler = optax.exponential_decay(optimisation_hyperparams['step_size'], \n",
    "                                        optimisation_hyperparams['decay_steps'], \n",
    "                                        optimisation_hyperparams['decay_factor'])\n",
    "\n",
    "    optimiser = optax.chain(optax.adamw(learning_rate = lr_scheduler, \n",
    "                            b1 = optimisation_hyperparams['adam_b1'],\n",
    "                            b2 = optimisation_hyperparams['adam_b2'],\n",
    "                            eps = optimisation_hyperparams['adam_eps'],\n",
    "                            weight_decay = optimisation_hyperparams['weight_decay']),\n",
    "                            optax.clip_by_global_norm(optimisation_hyperparams['max_grad_norm']))\n",
    "    \n",
    "    state = train_state.TrainState.create(apply_fn = model.apply, params = init_params, tx = optimiser)\n",
    "\n",
    "    return state, lr_scheduler\n",
    "\n",
    "def apply_model(state, data, hyperparams, key, A, x0):\n",
    "\n",
    "    return state.apply_fn({'params': {'encoder': state.params['encoder']}}, data, state.params['decoder'], hyperparams, key, A, x0)\n",
    "    # return state.apply_fn({'params': state.params}, data, state.params['decoder'], hyperparams, key, A, x0)\n",
    "\n",
    "batch_apply_model = vmap(apply_model, in_axes = (None, 0, None, 0, None, None))\n",
    "    \n",
    "def loss_fn(params, state, hyperparams, data, x0, kl_weight, key):\n",
    "\n",
    "    def cross_entropy_loss(hyperparams, p_xy_t, data):\n",
    "    \n",
    "        # compute the smooth maximum of the per-pixel bernoulli parameter across time steps\n",
    "        p_xy = np.sum(p_xy_t * nn.activation.softmax(p_xy_t * hyperparams['smooth_max_parameter'], axis = 0), axis = 0)\n",
    "\n",
    "        # compute the logit for each pixel\n",
    "        logits = np.log(p_xy / (1 - p_xy))\n",
    "\n",
    "        # compute the average cross entropy across pixels\n",
    "        cross_entropy = np.mean(optax.sigmoid_binary_cross_entropy(logits, data))\n",
    "\n",
    "        return cross_entropy\n",
    "\n",
    "    def KL_diagonal_Gaussians(mu_0, log_var_0, mu_1, log_var_1, hyperparams):\n",
    "        \"\"\"\n",
    "        KL(q||p), where q is posterior and p is prior\n",
    "        mu_0, log_var_0 is the mean and log variances of the prior\n",
    "        mu_1, log_var_1 is the mean and log variances of the posterior\n",
    "        var_min is added to the variances for numerical stability\n",
    "        \"\"\"\n",
    "        log_var_1 = stabilise_varance(log_var_1, hyperparams)\n",
    "\n",
    "        return np.sum(0.5 * (log_var_0 - log_var_1 + np.exp(log_var_1 - log_var_0) \n",
    "                             - 1.0 + (mu_1 - mu_0)**2 / np.exp(log_var_0)))\n",
    "\n",
    "    batch_cross_entropy_loss = vmap(cross_entropy_loss, in_axes = (None, 0, 0))\n",
    "    batch_KL_diagonal_Gaussians = vmap(KL_diagonal_Gaussians, in_axes = (None, None, 0, 0, None))\n",
    "\n",
    "    A = construct_dynamics_matrix(params['decoder'], hyperparams)\n",
    "    \n",
    "    # apply the model\n",
    "    batch_size = data.shape[0]\n",
    "    subkeys = random.split(key, batch_size)\n",
    "    results = batch_apply_model(state, data, hyperparams, subkeys, A, x0)\n",
    "\n",
    "    # calculate cross entropy\n",
    "    cross_entropy = batch_cross_entropy_loss(hyperparams, results['p_xy_t'], data).mean()\n",
    "\n",
    "    # calculate KL divergence between the approximate posterior and prior over the latents\n",
    "    mu_0 = 0\n",
    "    log_var_0 = params['prior_z_log_var']\n",
    "    mu_1 = results['z_mean']\n",
    "    log_var_1 = results['z_log_var']\n",
    "    kl_loss_prescale = batch_KL_diagonal_Gaussians(mu_0, log_var_0, mu_1, log_var_1, hyperparams).mean()\n",
    "    kl_loss = kl_weight * kl_loss_prescale\n",
    "\n",
    "    loss = cross_entropy + kl_loss\n",
    "\n",
    "    all_losses = {'total': loss, 'cross_entropy': cross_entropy, 'kl': kl_loss, 'kl_prescale': kl_loss_prescale}\n",
    "\n",
    "    return loss, all_losses\n",
    "\n",
    "loss_grad = value_and_grad(loss_fn, has_aux = True)\n",
    "eval_step_jit = jit(loss_fn)\n",
    "\n",
    "def train_step(state, hyperparams, training_data, x0, kl_weight, key):\n",
    "    \n",
    "    (loss, all_losses), grads = loss_grad(state.params, state, hyperparams, training_data, x0, kl_weight, key)\n",
    "\n",
    "    state = state.apply_gradients(grads = grads)\n",
    "    \n",
    "    gradient_fn = jax.value_and_grad(loss_fn, has_aux = True)\n",
    "    \n",
    "    return state, loss, all_losses\n",
    "\n",
    "train_step_jit = jit(train_step)\n",
    "\n",
    "def print_metrics(phase, duration, t_losses, v_losses, batch_range = [], lr = [], epoch = []):\n",
    "    \n",
    "    if phase == \"batch\":\n",
    "        \n",
    "        s1 = '\\033[1m' + \"Batches {}-{} in {:.2f} seconds, learning rate: {:.5f}\" + '\\033[0m'\n",
    "        print(s1.format(batch_range[0], batch_range[1], duration, lr))\n",
    "        \n",
    "    elif phase == \"epoch\":\n",
    "        \n",
    "        s1 = '\\033[1m' + \"Epoch {} in {:.1f} minutes\" + '\\033[0m'\n",
    "        print(s1.format(epoch, duration / 60))\n",
    "        \n",
    "    s2 = \"\"\"  Training losses {:.4f} = cross entropy {:.4f} + KL {:.4f} ({:.4f})\"\"\"\n",
    "    s3 = \"\"\"  Validation losses {:.4f} = cross entropy {:.4f} + KL {:.4f} ({:.4f})\"\"\"\n",
    "    s3 = \"\"\"  Validation losses {:.4f} = cross entropy {:.4f} + KL {:.4f} ({:.4f})\"\"\"\n",
    "    print(s2.format(t_losses['total'].mean(), t_losses['cross_entropy'].mean(),\n",
    "                    t_losses['kl'].mean(), t_losses['kl_prescale'].mean()))\n",
    "    print(s3.format(v_losses['total'].mean(), v_losses['cross_entropy'].mean(),\n",
    "                    v_losses['kl'].mean(), v_losses['kl_prescale'].mean()))\n",
    "    \n",
    "    if phase == \"epoch\":\n",
    "        print(\"\"\"\\n\"\"\")\n",
    "        \n",
    "def write_to_tensorboard(writer, t_losses, v_losses, epoch):\n",
    "\n",
    "    writer.scalar('loss (train)', t_losses['total'].mean(), epoch)\n",
    "    writer.scalar('cross entropy (train)', t_losses['cross_entropy'].mean(), epoch)\n",
    "    writer.scalar('KL (train)', t_losses['kl'].mean(), epoch)\n",
    "    writer.scalar('KL prescale (train)', t_losses['kl_prescale'].mean(), epoch)\n",
    "    writer.scalar('loss (validation)', v_losses['total'].mean(), epoch)\n",
    "    writer.scalar('cross entropy (validation)', v_losses['cross_entropy'].mean(), epoch)\n",
    "    writer.scalar('KL (validation)', v_losses['kl'].mean(), epoch)\n",
    "    writer.scalar('KL prescale (validation)', v_losses['kl_prescale'].mean(), epoch)\n",
    "    writer.flush()\n",
    "\n",
    "def optimise_VAE(init_params, x0, hyperparams, model, training_data, validation_data, \n",
    "                           optimisation_hyperparams, key, ckpt_dir, writer):\n",
    "\n",
    "    kl_schedule = kl_scheduler(optimisation_hyperparams)\n",
    "    \n",
    "    state, lr_scheduler = create_train_state(model, params, optimisation_hyperparams)\n",
    "    \n",
    "    # set early stopping criteria\n",
    "    early_stop = EarlyStopping(min_delta = optimisation_hyperparams['min_delta'], \n",
    "                               patience = optimisation_hyperparams['patience'])\n",
    "    \n",
    "    # loop over epochs\n",
    "    n_epochs = optimisation_hyperparams['n_epochs']\n",
    "    print_every = optimisation_hyperparams['print_every']\n",
    "    n_batches = optimisation_hyperparams['n_batches']\n",
    "    losses = {}\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # start epoch timer\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # convert the tf.data.Dataset training_data into an iterable\n",
    "        # this iterable is shuffled differently each epoch\n",
    "        train_datagen = iter(tfds.as_numpy(training_data))\n",
    "\n",
    "        # generate subkeys\n",
    "        key, *training_subkeys = random.split(key, num = n_batches + 1)\n",
    "        key, *validation_subkeys = random.split(key, num = int(n_batches / print_every) + 1)\n",
    "\n",
    "        # initialise the losses and the timer\n",
    "        training_losses = {'total': 0, 'cross_entropy': 0, 'kl': 0, 'kl_prescale': 0}\n",
    "        batch_start_time = time.time()\n",
    "\n",
    "        # loop over batches\n",
    "        for i in range(n_batches):\n",
    "\n",
    "            state, loss, all_losses = train_step_jit(state, hyperparams, np.array(next(train_datagen)['image']), \n",
    "                                                     x0, np.array(next(kl_schedule)), training_subkeys[i])\n",
    "\n",
    "            # training losses (average of 'print_every' batches)\n",
    "            training_losses = tree_map(lambda x, y: x + y / print_every, training_losses, all_losses)\n",
    "\n",
    "            if (i + 1) % print_every == 0:\n",
    "\n",
    "                # calculate loss on validation data\n",
    "                # _, validation_losses = eval_step_jit(state.params, state, hyperparams, validation_data, x0, kl_weight, \n",
    "                                                     # validation_subkeys[int((i + 1) / print_every) - 1])\n",
    "                validation_losses = training_losses\n",
    "                    \n",
    "                # end batches timer\n",
    "                batches_duration = time.time() - batch_start_time\n",
    "\n",
    "                # print metrics\n",
    "                print_metrics(\"batch\", batches_duration, training_losses, training_losses, \n",
    "                              batch_range = [i + 1 - print_every + 1, i + 1], lr = lr_scheduler(i + epoch * n_batches))\n",
    "\n",
    "                # store losses\n",
    "                if (i + 1) == print_every:\n",
    "                    \n",
    "                    t_losses_thru_training = copy(training_losses)\n",
    "                    v_losses_thru_training = copy(validation_losses)\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    t_losses_thru_training = tree_map(lambda x, y: np.append(x, y), t_losses_thru_training, training_losses)\n",
    "                    v_losses_thru_training = tree_map(lambda x, y: np.append(x, y), v_losses_thru_training, validation_losses)\n",
    "\n",
    "                # re-initialise the losses and timer\n",
    "                training_losses = {'total': 0, 'cross_entropy': 0, 'kl': 0, 'kl_prescale': 0}\n",
    "                batch_start_time = time.time()\n",
    "\n",
    "        losses['epoch ' + str(epoch)] = {'t_losses' : t_losses_thru_training, 'v_losses' : v_losses_thru_training}\n",
    "        \n",
    "        # end epoch timer\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        \n",
    "        # print losses (mean over all batches in epoch)\n",
    "        print_metrics(\"epoch\", epoch_duration, t_losses_thru_training, v_losses_thru_training, epoch = epoch + 1)\n",
    "\n",
    "        # write metrics to tensorboard\n",
    "        write_to_tensorboard(writer, t_losses_thru_training, v_losses_thru_training, epoch)\n",
    "        \n",
    "        # save checkpoint\n",
    "        ckpt = {'train_state': state, 'losses': losses, 'hyperparams': hyperparams, 'optimisation_hyperparams': optimisation_hyperparams}\n",
    "        checkpoints.save_checkpoint(ckpt_dir = ckpt_dir, target = ckpt, step = epoch)\n",
    "        \n",
    "        # if early stopping criteria met, break\n",
    "        _, early_stop = early_stop.update(v_losses_thru_training['total'].mean())\n",
    "        if early_stop.should_stop:\n",
    "            \n",
    "            print('Early stopping criteria met, breaking...')\n",
    "            \n",
    "            break\n",
    "            \n",
    "    return state, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aedcd70-655c-4fa2-9b32-fca3236ded0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching TensorBoard...\n",
      "Please visit http://localhost:6006 in a web browser.\n",
      "\u001b[1mBatches 1-1 in 16.42 seconds, learning rate: 0.00100\u001b[0m\n",
      "  Training losses 1.4123 = cross entropy 0.6687 + KL 0.7436 (74.3640)\n",
      "  Validation losses 1.4123 = cross entropy 0.6687 + KL 0.7436 (74.3640)\n",
      "\u001b[1mBatches 2-2 in 14.45 seconds, learning rate: 0.00100\u001b[0m\n",
      "  Training losses 1.4387 = cross entropy 0.6997 + KL 0.7390 (73.9018)\n",
      "  Validation losses 1.4387 = cross entropy 0.6997 + KL 0.7390 (73.9018)\n",
      "\u001b[1mBatches 3-3 in 11.61 seconds, learning rate: 0.00100\u001b[0m\n",
      "  Training losses 1.4506 = cross entropy 0.7069 + KL 0.7437 (74.3652)\n",
      "  Validation losses 1.4506 = cross entropy 0.7069 + KL 0.7437 (74.3652)\n",
      "\u001b[1mBatches 4-4 in 2.41 seconds, learning rate: 0.00100\u001b[0m\n",
      "  Training losses 1.4021 = cross entropy 0.6639 + KL 0.7382 (73.8201)\n",
      "  Validation losses 1.4021 = cross entropy 0.6639 + KL 0.7382 (73.8201)\n",
      "\u001b[1mBatches 5-5 in 2.43 seconds, learning rate: 0.00100\u001b[0m\n",
      "  Training losses 1.4944 = cross entropy 0.7566 + KL 0.7377 (73.7717)\n",
      "  Validation losses 1.4944 = cross entropy 0.7566 + KL 0.7377 (73.7717)\n",
      "\u001b[1mBatches 6-6 in 3.13 seconds, learning rate: 0.00100\u001b[0m\n",
      "  Training losses 1.4475 = cross entropy 0.7190 + KL 0.7286 (72.8581)\n",
      "  Validation losses 1.4475 = cross entropy 0.7190 + KL 0.7286 (72.8581)\n",
      "\u001b[1mBatches 7-7 in 2.96 seconds, learning rate: 0.00100\u001b[0m\n",
      "  Training losses 1.4313 = cross entropy 0.6996 + KL 0.7317 (73.1721)\n",
      "  Validation losses 1.4313 = cross entropy 0.6996 + KL 0.7317 (73.1721)\n",
      "\u001b[1mBatches 8-8 in 2.63 seconds, learning rate: 0.00100\u001b[0m\n",
      "  Training losses 1.4457 = cross entropy 0.7136 + KL 0.7320 (73.2009)\n",
      "  Validation losses 1.4457 = cross entropy 0.7136 + KL 0.7320 (73.2009)\n",
      "\u001b[1mBatches 9-9 in 3.01 seconds, learning rate: 0.00100\u001b[0m\n",
      "  Training losses 1.4140 = cross entropy 0.6812 + KL 0.7329 (73.2852)\n",
      "  Validation losses 1.4140 = cross entropy 0.6812 + KL 0.7329 (73.2852)\n",
      "\u001b[1mBatches 10-10 in 2.85 seconds, learning rate: 0.00100\u001b[0m\n",
      "  Training losses 1.4090 = cross entropy 0.6763 + KL 0.7327 (73.2719)\n",
      "  Validation losses 1.4090 = cross entropy 0.6763 + KL 0.7327 (73.2719)\n"
     ]
    }
   ],
   "source": [
    "from jax.config import config\n",
    "config.update(\"jax_debug_nans\", False)\n",
    "config.update(\"jax_disable_jit\", False)\n",
    "# use xeus-python kernel -- Python 3.9 (XPython) -- for debugging\n",
    "# typing help at a breakpoint() gives you list of available commands\n",
    "\n",
    "from flax.metrics import tensorboard\n",
    "log_folder = \"runs/exp9/profile\"\n",
    "writer = tensorboard.SummaryWriter(log_folder)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs/exp9\n",
    "\n",
    "ckpt_dir = 'tmp/flax-checkpointing'\n",
    "\n",
    "import shutil, os\n",
    "if os.path.exists(ckpt_dir):\n",
    "    shutil.rmtree(ckpt_dir) # remove any existing checkpoints from the last notebook run\n",
    "\n",
    "state, losses = \\\n",
    "optimise_VAE(init_params, x0, hyperparams, model, train_dataset, val_dataset, \n",
    "             optimisation_hyperparams, key, ckpt_dir, writer)\n",
    "\n",
    "# # restore checkpoint\n",
    "# ckpt = {'train_state': state, 'losses': losses, 'hyperparams': hyperparams, 'optimisation_hyperparams': optimisation_hyperparams}\n",
    "# restored_state = checkpoints.restore_checkpoint(ckpt_dir = ckpt_dir, target = ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ab104-b63e-4763-aa58-63bdc59e73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run model and plot\n",
    "# T = 100\n",
    "# data = training_data[0,:,:]\n",
    "# hyperparams['dt'] = 0.01\n",
    "# hyperparams['time_steps'] = 1000\n",
    "\n",
    "# params = init_params\n",
    "# results_encode = encode(models, params, data[None,:,:])\n",
    "# _, z_sampler, _ = models\n",
    "# z1, z2 = z_sampler(results_encode, params, hyperparams, key)\n",
    "# A = construct_dynamics_matrix(params, hyperparams)\n",
    "# results_decode = batch_decode(params, hyperparams, models, A, x0, T, z1, z2)\n",
    "# results = results_encode | results_decode\n",
    "\n",
    "# for ex in range(1):\n",
    "#     plt.scatter(results['pen_xy'][ex][:,0],results['pen_xy'][ex][:,1], c = 'k', alpha =  np.exp(results['pen_down_log_p'][ex,:]))\n",
    "# plt.ylim(0,105)\n",
    "# plt.xlim(0,105)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd497aae-5366-4e41-bc58-27188c6c6d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of time when running model forward is decoder and x-entropy functions\n",
    "\n",
    "# replace cnn with capsule?\n",
    "# do i need to standardize images (and how) or is normalised (as is) fine?\n",
    "# ultimately replace pen with myosuite finger to make it a finger painting task - could have flexion/extension determine finger up/down\n",
    "\n",
    "# pen_xy0 options: always start in center of canvas (105/2, 105/2), randomise and use extra feature dimension in image, let the CNN choose\n",
    "# consider learning initial neural states\n",
    "# add control costs? e.g. squared velocity costs?\n",
    "# add batch norm and other tricks to CNN, dropout?\n",
    "\n",
    "# if there are l layers (e.g. 3), the top-layer alphas on the last l-1 time steps (e.g. 2) don't influence the state of the lowest layer and hence the objective\n",
    "# when n_layer = 3, top-layer alphas at time 1 inlfuence the state of the lowest layer and hence the objective at time 3, and so on\n",
    "# this leads to zeros in the biases and columns of weight matrix in last dense layer\n",
    "\n",
    "# relu activation function, and to some extent binary image data, can lead to zero gradients scattered throughout CNN\n",
    "# these gradients go to zero if you change relu to tanh and make data continuous on [0, 1], so not pathological, i don't think\n",
    "\n",
    "# print values without tracer information\n",
    "# jax.debug.print(\"{z_mean}\", z_mean = z_mean)\n",
    "# jax.debug.breakpoint() - didn't work for me"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
