{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7c2a048-fa01-47d7-b692-7ad1426df587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random, lax, jit, vmap, value_and_grad\n",
    "from jax.tree_util import tree_map\n",
    "from jax.nn import initializers\n",
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "from flax import linen as nn\n",
    "\n",
    "import optax\n",
    "from flax.training import checkpoints\n",
    "from flax.training.early_stopping import EarlyStopping\n",
    "\n",
    "import time\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "658e9930-284e-4f27-8f32-6f4f67679b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    n_loops_top_layer: int\n",
    "    x_dim_top_layer: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # CNN based on End-to-End Training of Deep Visuomotor Policies\n",
    "        x = nn.Conv(features = 64, kernel_size = (7, 7))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features = 32, kernel_size = (5, 5))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features = 32, kernel_size = (5, 5))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = x.reshape((x.shape[0], -1)) # flatten\n",
    "        x = nn.Dense(features = 64)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features = 40)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features = 40)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features = (self.n_loops_top_layer + self.x_dim_top_layer) * 2)(x)\n",
    "        \n",
    "        # # CNN based on deepmimic\n",
    "        # x = nn.Conv(features = 16, kernel_size = (8, 8))(x)\n",
    "        # x = nn.relu(x)\n",
    "        # x = nn.Conv(features = 32, kernel_size = (4, 4))(x)\n",
    "        # x = nn.relu(x)\n",
    "        # x = nn.Conv(features = 32, kernel_size = (4, 4))(x)\n",
    "        # x = nn.relu(x)\n",
    "        # x = x.reshape((x.shape[0], -1)) # flatten\n",
    "        # x = nn.Dense(features = 64)(x)\n",
    "        # x = nn.relu(x)\n",
    "        # x = nn.Dense(features = 1024)(x)\n",
    "        # x = nn.relu(x)\n",
    "        # x = nn.Dense(features = 512)(x)\n",
    "        # x = nn.relu(x)\n",
    "        # x = nn.Dense(features = (self.n_loops_top_layer + self.x_dim_top_layer) * 2)(x)\n",
    "        \n",
    "        # mean and log variances of Gaussian distribution over latents\n",
    "        z_mean, z_log_var = np.split(x, 2, axis = 1)\n",
    "        \n",
    "        return z_mean, z_log_var\n",
    "    \n",
    "class pen_actions_readout(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # linear layer\n",
    "        outputs = nn.Dense(3)(x)\n",
    "\n",
    "        # pen velocity in x and y directions\n",
    "        d_xy = outputs[:2]\n",
    "\n",
    "        # log probability that the pen is down (actively drawing)\n",
    "        pen_down_log_p = nn.log_sigmoid(outputs[2])\n",
    "    \n",
    "        return d_xy, pen_down_log_p\n",
    "    \n",
    "class sample_latents():\n",
    "    \n",
    "    def __init__(self, n_loops_top_layer):\n",
    "        self.n_loops_top_layer = n_loops_top_layer\n",
    "    \n",
    "    def __call__(self, results, params, hyperparams, key):\n",
    "\n",
    "        # sample the latents\n",
    "        z = diag_Gaussian_sample(results['z_mean'], results['z_log_var'], hyperparams, key)\n",
    "\n",
    "        # split into latents that determine the top-layer alphas (z1) and initial state (z2)\n",
    "        z1, z2 = np.split(z, [self.n_loops_top_layer], axis = 1)\n",
    "\n",
    "        return nn.activation.softmax(z1 / params['dynamics']['t'][0], axis = 1), z2\n",
    "    \n",
    "def update_pen_position(pen_xy, d_xy, hyperparams):\n",
    "    \n",
    "    # candidate new pen position\n",
    "    pen_xy = pen_xy + d_xy\n",
    "    \n",
    "    # align pen position relative to centre of canvas\n",
    "    pen_xy = pen_xy - hyperparams['image_dim'] / 2\n",
    "    \n",
    "    # transform canvas boundaries to -/+ 5\n",
    "    pen_xy = pen_xy * 2 / hyperparams['image_dim'] * 5\n",
    "    \n",
    "    # squash pen position to be within canvas boundaries\n",
    "    pen_xy = nn.sigmoid(pen_xy)\n",
    "    \n",
    "    # transform canvas boundaries back to their original values\n",
    "    pen_xy_new = pen_xy * hyperparams['image_dim']\n",
    "    \n",
    "    return pen_xy_new\n",
    "\n",
    "def decode_one_step(params, hyperparams, models, A, state, inputs):\n",
    "    \n",
    "    x, pen_xy = state\n",
    "    \n",
    "    top_layer_alphas = inputs\n",
    "    \n",
    "    # compute the alphas, softmax(W @ x + b, t)\n",
    "    alphas = jax.tree_map(lambda W, x, b, t: nn.activation.softmax((W @ x + b) / t, axis = 0), \n",
    "                          params['dynamics']['W_a'], x[:2], params['dynamics']['b'], params['dynamics']['t'][1:])\n",
    "    \n",
    "    # prepend the top-layer alphas\n",
    "    alphas.insert(0, top_layer_alphas)\n",
    "    \n",
    "    # compute the additive inputs\n",
    "    u = jax.tree_map(lambda W, x: W @ x, params['dynamics']['W_u'], x[:2])\n",
    "    \n",
    "    # prepend the top-layer additive inputs\n",
    "    u.insert(0, x[0] * 0)\n",
    "\n",
    "    # update the states\n",
    "    x_new = jax.tree_map(lambda A, x, alphas, u: x + (np.sum(alphas[:, None, None] * A, axis = 0) @ x + u) * hyperparams['dt'], A, x, alphas, u)\n",
    "\n",
    "    # readout the pen actions (pen velocity and pen down probability) from the state at the bottom layer\n",
    "    _, _, readout = models\n",
    "    d_xy, pen_down_log_p = readout.apply(params['readout'], x = x_new[-1])\n",
    "    \n",
    "    # calculate the per-pixel bernoulli parameter\n",
    "    p_xy = per_pixel_bernoulli_parameter(params, hyperparams, pen_xy, pen_down_log_p)\n",
    "    \n",
    "    # update the pen position based on the pen velocity\n",
    "    pen_xy_new = update_pen_position(pen_xy, d_xy, hyperparams)\n",
    "\n",
    "    state = x_new, pen_xy_new\n",
    "    outputs = alphas, x_new, pen_xy_new, p_xy, pen_down_log_p\n",
    "    \n",
    "    return state, outputs\n",
    "\n",
    "def decode(params, hyperparams, models, A, x0, T, z1, z2):\n",
    "\n",
    "    x0[0] = z2\n",
    "    pen_xy0 = hyperparams['image_dim'] / 2 # initialise pen in centre of canvas\n",
    "\n",
    "    decoder = lambda state, inputs: decode_one_step(params, hyperparams, models, A, state = state, inputs = inputs)\n",
    "    _, (alphas, x, pen_xy, p_xy_t, pen_down_log_p) = lax.scan(decoder, (x0, pen_xy0), np.repeat(z1[None,:], T, axis = 0))\n",
    "\n",
    "    return {'alphas': alphas,\n",
    "            'x0': x0,\n",
    "            'x': x,\n",
    "            'pen_xy0': pen_xy0,\n",
    "            'pen_xy': pen_xy,\n",
    "            'p_xy_t': p_xy_t,\n",
    "            'pen_down_log_p': pen_down_log_p}\n",
    "\n",
    "batch_decode = vmap(decode, in_axes = (None, None, None, None, None, None, 0, 0))\n",
    "\n",
    "def encode(models, params, data):\n",
    "\n",
    "    encoder, _, _ = models\n",
    "    \n",
    "    z_mean, z_log_var = encoder.apply(params['encoder'], x = data)\n",
    "\n",
    "    return {'z_mean': z_mean, \n",
    "            'z_log_var': z_log_var}\n",
    "    \n",
    "def losses(params, hyperparams, models, data, x0, T, kl_weight, key):\n",
    "    \n",
    "    # pass the data through the encoder\n",
    "    results_encode = encode(models, params, data)\n",
    "    \n",
    "    # sample latent variables from the approximate posterior\n",
    "    _, z_sampler, _ = models\n",
    "    z1, z2 = z_sampler(results_encode, params, hyperparams, key)\n",
    "\n",
    "    # pass the latent variables through the decoder\n",
    "    A = construct_dynamics_matrix(params, hyperparams)\n",
    "    results_decode = batch_decode(params, hyperparams, models, A, x0, T, z1, z2)\n",
    "    \n",
    "    results = results_encode | results_decode\n",
    "\n",
    "    # cross entropy given latent variables\n",
    "    cross_entropy = batch_cross_entropy_loss(params, hyperparams, results['p_xy_t'], data).mean()\n",
    "    \n",
    "    # KL divergence between the approximate posterior and prior over the latents\n",
    "    mu_0 = 0\n",
    "    log_var_0 = params['prior_z_log_var']\n",
    "    mu_1 = results['z_mean']\n",
    "    log_var_1 = results['z_log_var']\n",
    "    kl_loss_prescale = batch_KL_diagonal_Gaussians(mu_0, log_var_0, mu_1, log_var_1, hyperparams).mean()\n",
    "    kl_loss = kl_weight * kl_loss_prescale\n",
    "    \n",
    "    loss = cross_entropy + kl_loss\n",
    "    \n",
    "    all_losses = {'total': loss, 'cross_entropy': cross_entropy, 'kl': kl_loss, 'kl_prescale': kl_loss_prescale}\n",
    "    \n",
    "    return loss, all_losses\n",
    "\n",
    "losses_jit = jit(losses, static_argnums = (2, 5))\n",
    "training_loss_grad = value_and_grad(losses_jit, has_aux = True)\n",
    "\n",
    "def construct_dynamics_matrix(params, hyperparams):\n",
    "    \n",
    "    # positive semi-definite matrix P\n",
    "    P = jax.tree_map(lambda P: P @ P.T, params['dynamics']['P'])\n",
    "\n",
    "    # skew symmetric matrix S\n",
    "    S = jax.tree_map(lambda U, V: U @ np.transpose(V, (0, 2, 1)) - V @ np.transpose(U, (0, 2, 1)), \n",
    "                     params['dynamics']['S_U'], params['dynamics']['S_V'])\n",
    "\n",
    "    # dynamics matrix A (loops organised along axis 0)\n",
    "    A = jax.tree_map(lambda L, S, P: (-L @ np.transpose(L, (0, 2, 1)) + S) @ P, params['dynamics']['L'], S, P)\n",
    "\n",
    "    return A\n",
    "\n",
    "def stabilise_varance(log_var, hyperparams):\n",
    "    \"\"\"\n",
    "    var_min is added to the variances for numerical stability\n",
    "    \"\"\"\n",
    "    return np.log(np.exp(log_var) + hyperparams['var_min'])\n",
    "\n",
    "def diag_Gaussian_sample(mean, log_var, hyperparams, key):\n",
    "    \"\"\"\n",
    "    sample from a diagonal Gaussian distribution\n",
    "    \"\"\"\n",
    "    log_var = stabilise_varance(log_var, hyperparams)\n",
    "    \n",
    "    return mean + np.exp(0.5 * log_var) * random.normal(key, mean.shape)\n",
    "\n",
    "def log_Gaussian_kernel(x, mu, log_var, hyperparams):\n",
    "    \"\"\"\n",
    "    calculate the log likelihood of x under a diagonal Gaussian distribution\n",
    "    \"\"\"\n",
    "    log_var = stabilise_varance(log_var, hyperparams)\n",
    "    \n",
    "    return -0.5 * (x - mu)**2 / np.exp(log_var)\n",
    "\n",
    "def log_likelihood_diagonal_Gaussian(x, mu, log_var, hyperparams):\n",
    "    \"\"\"\n",
    "    calculate the log likelihood of x under a diagonal Gaussian distribution\n",
    "    \"\"\"\n",
    "    log_var = stabilise_varance(log_var, hyperparams)\n",
    "    \n",
    "    return np.sum(-0.5 * (log_var + np.log(2 * np.pi) + (x - mu)**2 / np.exp(log_var)))\n",
    "\n",
    "def per_pixel_bernoulli_parameter(params, hyperparams, pen_xy, pen_down_log_p):\n",
    "    \n",
    "    ll_p_x = log_Gaussian_kernel(pen_xy[0], hyperparams['x_pixels'], params['pen_log_var'], hyperparams)\n",
    "    ll_p_y = log_Gaussian_kernel(pen_xy[1], hyperparams['y_pixels'], params['pen_log_var'], hyperparams)\n",
    "    \n",
    "    p_xy_t = np.exp(ll_p_x[None,:] + ll_p_y[:,None] + pen_down_log_p)\n",
    "    \n",
    "    return p_xy_t\n",
    "\n",
    "def cross_entropy_loss(params, hyperparams, p_xy_t, data):\n",
    "    \n",
    "    # compute the smooth maximum of the per-pixel bernoulli parameter across time steps\n",
    "    p_xy = np.sum(p_xy_t * nn.activation.softmax(p_xy_t * hyperparams['smooth_max_parameter'], axis = 0), axis = 0)\n",
    "\n",
    "    # compute the logit for each pixel\n",
    "    logits = np.log(p_xy / (1 - p_xy))\n",
    "    \n",
    "    # compute the average cross entropy across pixels\n",
    "    cross_entropy = np.mean(optax.sigmoid_binary_cross_entropy(logits, data))\n",
    "\n",
    "    return cross_entropy\n",
    "\n",
    "batch_cross_entropy_loss = vmap(cross_entropy_loss, in_axes = (None, None, 0, 0))\n",
    "\n",
    "def KL_diagonal_Gaussians(mu_0, log_var_0, mu_1, log_var_1, hyperparams):\n",
    "    \"\"\"\n",
    "    KL(q||p), where q is posterior and p is prior\n",
    "    mu_0, log_var_0 is the mean and log variances of the prior\n",
    "    mu_1, log_var_1 is the mean and log variances of the posterior\n",
    "    var_min is added to the variances for numerical stability\n",
    "    \"\"\"\n",
    "    log_var_1 = stabilise_varance(log_var_1, hyperparams)\n",
    "    \n",
    "    return np.sum(0.5 * (log_var_0 - log_var_1 + np.exp(log_var_1 - log_var_0) \n",
    "                         - 1.0 + (mu_1 - mu_0)**2 / np.exp(log_var_0)))\n",
    "\n",
    "batch_KL_diagonal_Gaussians = vmap(KL_diagonal_Gaussians, in_axes = (None, None, 0, 0, None))\n",
    "\n",
    "def keyGen(key, n_subkeys):\n",
    "    \n",
    "    keys = random.split(key, n_subkeys + 1)\n",
    "    \n",
    "    return keys[0], (k for k in keys[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43d39ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_dynamics_parameters(hyperparams,  key):\n",
    "    \n",
    "    P = []\n",
    "    S_U = []\n",
    "    S_V = []\n",
    "    L = []\n",
    "    W_u = []\n",
    "    W_a = []\n",
    "    b = []\n",
    "    t = []\n",
    "    \n",
    "    n_layers = len(hyperparams['x_dim'])\n",
    "    for layer in range(n_layers):\n",
    "        \n",
    "        key, subkeys = keyGen(key, n_subkeys = 6)\n",
    "        \n",
    "        n_loops = hyperparams['n_loops'][layer]\n",
    "        x_dim = hyperparams['x_dim'][layer]\n",
    "        \n",
    "        # parameters of layer-specific P\n",
    "        p = random.normal(next(subkeys), (x_dim, x_dim))\n",
    "        \n",
    "        # set trace of P @ P.T to x_dim\n",
    "        P.append(p * np.sqrt(x_dim / np.trace(p @ p.T)))\n",
    "        \n",
    "        # parameters of layer- and loop-specific S\n",
    "        u = random.normal(next(subkeys), (n_loops, x_dim, int(x_dim / n_loops)))\n",
    "        v = random.normal(next(subkeys), (n_loops, x_dim, int(x_dim / n_loops)))\n",
    "        \n",
    "        # set variance of elements of S to 1/(n_loops * x_dim)\n",
    "        s = u @ np.transpose(v, (0, 2, 1)) - v @ np.transpose(u, (0, 2, 1))\n",
    "        # f = 1 / np.linalg.norm(s, axis = (1, 2))[:, None, None] / n_loops # frobenius norm of each loop 1/n_loops\n",
    "        f = 1 / np.std(s, axis = (1, 2))[:, None, None] / np.sqrt(n_loops * x_dim)\n",
    "        S_U.append(u * np.sqrt(f))\n",
    "        S_V.append(v * np.sqrt(f))\n",
    "\n",
    "        # parameters of layer- and loop-specific L\n",
    "        Q, _ = np.linalg.qr(random.normal(next(subkeys), (x_dim, x_dim)))\n",
    "        L_i = np.split(Q * np.sqrt(n_loops), n_loops, axis = 1)\n",
    "        L.append(np.stack(L_i, axis = 0))\n",
    "\n",
    "        # parameters of the mapping from hidden states to alphas, alphas = softmax(W @ x + b, temperature)\n",
    "        if layer != 0:\n",
    "            \n",
    "            # weights for additive inputs\n",
    "            std_W = 1 / np.sqrt(hyperparams['x_dim'][layer - 1])\n",
    "            W_u.append(random.normal(next(subkeys), (hyperparams['x_dim'][layer], hyperparams['x_dim'][layer - 1])) * std_W)\n",
    "            \n",
    "            # weights for modulatory factors\n",
    "            W_a.append(random.normal(next(subkeys), (n_loops, hyperparams['x_dim'][layer - 1])) * std_W)\n",
    "\n",
    "            # bias for modulatory factors\n",
    "            b.append(np.zeros((n_loops)))\n",
    "            \n",
    "        # temperature of layer-specific softmax function\n",
    "        t.append(1.0)\n",
    "\n",
    "    return {'P': P, \n",
    "            'S_U': S_U,\n",
    "            'S_V': S_V, \n",
    "            'L': L, \n",
    "            'W_u': W_u,\n",
    "            'W_a': W_a, \n",
    "            'b': b,\n",
    "            't': t}\n",
    "\n",
    "def initialise_hidden_states(hyperparams):\n",
    "              \n",
    "    # initialise the hidden states of the decoder model to zero (not learned)\n",
    "    n_layers = len(hyperparams['x_dim'])\n",
    "    init_states = []\n",
    "    for layer in range(n_layers):\n",
    "        init_states.append(np.zeros(hyperparams['x_dim'][layer]))\n",
    "        \n",
    "    return init_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1187743d-c012-46ba-9698-23fd82085055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random as rdm\n",
    "from sys import platform as sys_pf\n",
    "import matplotlib\n",
    "if sys_pf == 'darwin':\n",
    "    matplotlib.use(\"TkAgg\")\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def num2str(idx):\n",
    "    if idx < 10:\n",
    "        return '0'+str(idx)\n",
    "    return str(idx)\n",
    "\n",
    "def load_img(fn):\n",
    "    I = plt.imread(fn)\n",
    "    I = np.array(I,dtype=bool)\n",
    "    return I\n",
    "\n",
    "def load_motor(fn):\n",
    "    motor = []\n",
    "    with open(fn,'r') as fid:\n",
    "        lines = fid.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    for myline in lines:\n",
    "        if myline =='START': # beginning of character\n",
    "            stk = []\n",
    "        elif myline =='BREAK': # break between strokes\n",
    "            stk = np.array(stk)\n",
    "            motor.append(stk) # add to list of strokes\n",
    "            stk = [] \n",
    "        else:\n",
    "            arr = np.fromstring(myline,dtype=float,sep=',')\n",
    "            stk.append(arr)\n",
    "    return motor\n",
    "\n",
    "img_dir = '/Users/James/Dropbox/James MacBook/Guillaume/omniglot/omniglot/python/images_background'\n",
    "stroke_dir = '/Users/James/Dropbox/James MacBook/Guillaume/omniglot/omniglot/python/strokes_background'\n",
    "pth_i = os.path.join(img_dir, 'Sanskrit')\n",
    "pth_s = os.path.join(stroke_dir, 'Sanskrit')\n",
    "n_characters = len([s for s in os.listdir(pth_i) if 'character' in s])\n",
    "n_reps_per_character = 20\n",
    "training_data = onp.empty((n_characters * n_reps_per_character, 105, 105))\n",
    "for character in range(1):\n",
    "    \n",
    "    # get directories for this character\n",
    "    img_char_dir = os.path.join(pth_i, 'character' + num2str(character + 1))\n",
    "    stroke_char_dir = os.path.join(pth_s, 'character' + num2str(character + 1))\n",
    "    \n",
    "    # get base file name for this character\n",
    "    # print(os.listdir(img_char_dir)[0])\n",
    "    fn_example = os.listdir(img_char_dir)[0]\n",
    "    fn_base = fn_example[:fn_example.find('_')] \n",
    "    \n",
    "    for rep in range(1):\n",
    "        \n",
    "        fn_img = img_char_dir + '/' + fn_base + '_' + num2str(rep + 1) + '.png'\n",
    "        I = load_img(fn_img) == False # ensures letter pixels are 1\n",
    "        training_data[character * n_reps_per_character + rep,:,:] = I\n",
    "        \n",
    "        fn_stk = stroke_char_dir + '/' + fn_base + '_' + num2str(rep + 1) + '.txt'\n",
    "        # motor = load_motor(fn_stk)\n",
    "\n",
    "training_data = np.array(training_data)\n",
    "\n",
    "# # plot images and strokes\n",
    "# plt.imshow(training_data[0,:,:], cmap = 'binary')\n",
    "# plt.show()\n",
    "# for ex in range(1):\n",
    "#     for m in motor:\n",
    "#         plt.scatter(m[:,0],m[:,1], c = 'k')\n",
    "# plt.ylim(-105, 0)\n",
    "# plt.xlim(0, 105)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fca12b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly generate a PRNG key\n",
    "seed = 0\n",
    "key = random.PRNGKey(seed)\n",
    "\n",
    "# generate the required number of subkeys\n",
    "key, subkeys = keyGen(key, n_subkeys = 5)    \n",
    "\n",
    "image_height = 105\n",
    "image_width = 105\n",
    "validation_data = training_data[0:20,:,:]\n",
    "\n",
    "# primary hyperparameters\n",
    "hyperparams = {'image_dim': np.array(training_data.shape[1:]),\n",
    "               'x_dim': [20, 50, 200],\n",
    "               'alpha_fraction': 0.1,\n",
    "               'time_steps': 1000,\n",
    "               'dt': 0.01,\n",
    "               'var_min': 1e-16,\n",
    "               'smooth_max_parameter': 1e3}\n",
    "\n",
    "# secondary hyperparameters (derived from primary hyperparameters)\n",
    "hyperparams['n_loops'] = [int(np.ceil(i * hyperparams['alpha_fraction'])) for i in hyperparams['x_dim']]\n",
    "hyperparams['x_pixels'] = np.linspace(0.5, hyperparams['image_dim'][1] - 0.5, hyperparams['image_dim'][1])\n",
    "hyperparams['y_pixels'] = np.linspace(0.5, hyperparams['image_dim'][0] - 0.5, hyperparams['image_dim'][0])\n",
    "\n",
    "encoder = CNN(n_loops_top_layer = hyperparams['n_loops'][0], x_dim_top_layer = hyperparams['x_dim'][0])\n",
    "z_sampler = sample_latents(n_loops_top_layer = hyperparams['n_loops'][0])\n",
    "readout = pen_actions_readout()\n",
    "models = (encoder, z_sampler, readout)\n",
    "\n",
    "init_params = {}\n",
    "init_params['encoder'] = encoder.init(x = np.ones((1, training_data.shape[1], training_data.shape[2])), \n",
    "                                      rngs = {'params': next(subkeys)})\n",
    "init_params['prior_z_log_var'] = np.log(0.1)\n",
    "init_params['dynamics'] = initialise_dynamics_parameters(hyperparams, next(subkeys))\n",
    "init_params['readout'] = readout.init(x = np.ones((hyperparams['x_dim'][-1])), rngs = {'params': next(subkeys)})\n",
    "init_params['pen_log_var'] = 10.0\n",
    "\n",
    "x0 = initialise_hidden_states(hyperparams)\n",
    "\n",
    "optimisation_hyperparams = {'kl_warmup_start': 500,\n",
    "                            'kl_warmup_end': 1000,\n",
    "                            'kl_min': 0.01,\n",
    "                            'kl_max': 1,\n",
    "                            'adam_b1': 0.9,\n",
    "                            'adam_b2': 0.999,\n",
    "                            'adam_eps': 1e-8,\n",
    "                            'weight_decay': 0.0001,\n",
    "                            'max_grad_norm': 10,\n",
    "                            'step_size': 0.001,\n",
    "                            'decay_steps': 1,\n",
    "                            'decay_factor': 0.9999,\n",
    "                            'batch_size': 5,\n",
    "                            'print_every': 1,\n",
    "                            'n_epochs': 10,\n",
    "                            'min_delta': 1e-3,\n",
    "                            'patience': 2}\n",
    "optimisation_hyperparams['n_batches'] = int(training_data.shape[0] / optimisation_hyperparams['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a0c17d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_scheduler(optimisation_hyperparams):\n",
    "    \n",
    "    kl_warmup_start = optimisation_hyperparams['kl_warmup_start']\n",
    "    kl_warmup_end = optimisation_hyperparams['kl_warmup_end']\n",
    "    kl_min = optimisation_hyperparams['kl_min']\n",
    "    kl_max = optimisation_hyperparams['kl_max']\n",
    "    n_batches = optimisation_hyperparams['n_batches']\n",
    "    \n",
    "    kl_schedule = []\n",
    "    for i_batch in range(n_batches):\n",
    "        \n",
    "        warm_up_fraction = min(max((i_batch - kl_warmup_start) / (kl_warmup_end - kl_warmup_start),0),1)\n",
    "        \n",
    "        kl_schedule.append(kl_min + warm_up_fraction * (kl_max - kl_min))\n",
    "\n",
    "    return np.array(kl_schedule)\n",
    "\n",
    "def reshape_training_data(training_data):\n",
    "    \n",
    "    return np.reshape(training_data, (optimisation_hyperparams['n_batches'],\n",
    "                                      optimisation_hyperparams['batch_size'],\n",
    "                                      training_data.shape[1],\n",
    "                                      training_data.shape[2]))\n",
    "\n",
    "def optimize_dynamical_VAE_core(params, x0, hyperparams, models, training_data, validation_data, optimizer, optimizer_state, \n",
    "                                optimisation_hyperparams, kl_schedule, print_every, epoch, scheduler, key):\n",
    "    \n",
    "    n_batches = optimisation_hyperparams['n_batches']\n",
    "    \n",
    "    # generate subkeys\n",
    "    key, training_subkeys = keyGen(key, n_subkeys = n_batches)\n",
    "    key, validation_subkeys = keyGen(key, n_subkeys = int(n_batches / print_every))\n",
    "    \n",
    "    # initialise the losses and the timer\n",
    "    training_losses = {'total': 0, 'cross_entropy': 0, 'kl': 0, 'kl_prescale': 0}\n",
    "    start_time = time.time()\n",
    "\n",
    "    # loop over batches\n",
    "    for i in range(n_batches):\n",
    "        \n",
    "        i_batch = i + epoch * n_batches\n",
    "\n",
    "        kl_weight = kl_schedule[i_batch]\n",
    "        \n",
    "        (loss, all_losses), grad = training_loss_grad(params, hyperparams, models, training_data[i],\n",
    "                                                      x0, hyperparams['time_steps'], kl_weight, next(training_subkeys)) \n",
    "\n",
    "        updates, optimizer_state = optimizer.update(grad, optimizer_state, params)\n",
    "\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        \n",
    "        # training losses (average of 'print_every' batches)\n",
    "        training_losses = tree_map(lambda x, y: x + y / print_every, training_losses, all_losses)\n",
    "        \n",
    "        if (i + 1) % print_every == 0:\n",
    "        \n",
    "            # calculate loss on validation data\n",
    "            _, validation_losses = losses_jit(params, hyperparams, models, validation_data, x0, \n",
    "                                              hyperparams['time_steps'], kl_weight, next(validation_subkeys))\n",
    "            \n",
    "            # end batches timer\n",
    "            batches_duration = time.time() - start_time\n",
    "\n",
    "            # print and store losses\n",
    "            s1 = '\\033[1m' + \"Batches {}-{} in {:.2f} seconds, step size: {:.5f}\" + '\\033[0m'\n",
    "            s2 = \"\"\"  Training losses {:.4f} = cross entropy {:.4f} + KL {:.4f} ({:.4f})\"\"\"\n",
    "            s3 = \"\"\"  Validation losses {:.4f} = cross entropy {:.4f} + KL {:.4f} ({:.4f})\"\"\"\n",
    "            print(s1.format(i + 1 - print_every + 1, i + 1, batches_duration, scheduler(i_batch)))\n",
    "            print(s2.format(training_losses['total'], training_losses['cross_entropy'],\n",
    "                            training_losses['kl'], training_losses['kl_prescale']))\n",
    "            print(s3.format(validation_losses['total'], validation_losses['cross_entropy'],\n",
    "                            validation_losses['kl'], validation_losses['kl_prescale']))\n",
    "\n",
    "            if (i + 1) == print_every:\n",
    "                tlosses_thru_training = copy(training_losses)\n",
    "                vlosses_thru_training = copy(validation_losses)\n",
    "            else:\n",
    "                tlosses_thru_training = tree_map(lambda x, y: np.append(x, y), tlosses_thru_training, training_losses)\n",
    "                vlosses_thru_training = tree_map(lambda x, y: np.append(x, y), vlosses_thru_training, validation_losses)\n",
    "            \n",
    "            # re-initialise the losses and timer\n",
    "            training_losses = {'total': 0, 'cross_entropy': 0, 'kl': 0, 'kl_prescale': 0}\n",
    "            start_time = time.time()\n",
    "            \n",
    "    losses = {'tlosses' : tlosses_thru_training, 'vlosses' : vlosses_thru_training}\n",
    "\n",
    "    return params, optimizer_state, losses\n",
    "\n",
    "def optimize_dynamical_VAE(params, x0, hyperparams, models, training_data, validation_data, \n",
    "                           optimisation_hyperparams, key, ckpt_dir, writer):\n",
    "\n",
    "    kl_schedule = kl_scheduler(optimisation_hyperparams)\n",
    "    \n",
    "    scheduler = optax.exponential_decay(optimisation_hyperparams['step_size'], \n",
    "                                        optimisation_hyperparams['decay_steps'], \n",
    "                                        optimisation_hyperparams['decay_factor'])\n",
    "\n",
    "    optimizer = optax.chain(optax.adamw(learning_rate = scheduler, \n",
    "                            b1 = optimisation_hyperparams['adam_b1'],\n",
    "                            b2 = optimisation_hyperparams['adam_b2'],\n",
    "                            eps = optimisation_hyperparams['adam_eps'],\n",
    "                            weight_decay = optimisation_hyperparams['weight_decay']),\n",
    "                            optax.clip_by_global_norm(optimisation_hyperparams['max_grad_norm']))\n",
    "    \n",
    "    optimizer_state = optimizer.init(params)\n",
    "    \n",
    "    # reshape training data\n",
    "    training_data = reshape_training_data(training_data)\n",
    "    \n",
    "    # set early stopping criteria\n",
    "    early_stop = EarlyStopping(min_delta = optimisation_hyperparams['min_delta'], \n",
    "                               patience = optimisation_hyperparams['patience'])\n",
    "    \n",
    "    # loop over epochs\n",
    "    n_epochs = optimisation_hyperparams['n_epochs']\n",
    "    print_every = optimisation_hyperparams['print_every']\n",
    "    losses = {}\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # start epoch timer\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # generate subkeys\n",
    "        key, subkeys = keyGen(key, n_subkeys = 2)\n",
    "        \n",
    "        # shuffle the batches every epoch\n",
    "        data = random.permutation(next(subkeys), training_data, axis = 0)\n",
    "        \n",
    "        # perform optimisation\n",
    "        params, optimizer_state, losses['epoch ' + str(epoch)] = \\\n",
    "        optimize_dynamical_VAE_core(params, x0, hyperparams, models, data, validation_data, optimizer, optimizer_state, \n",
    "                                    optimisation_hyperparams, kl_schedule, print_every, epoch, scheduler, next(subkeys))\n",
    "        \n",
    "        # end epoch timer\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        \n",
    "        # print metrics (mean over all batches in epoch) \n",
    "        s1 = '\\033[1m' + \"Epoch {} in {:.1f} minutes\" + '\\033[0m'\n",
    "        s2 = \"\"\"  Training losses {:.4f} = cross entropy {:.4f} + KL {:.4f} ({:.4f})\"\"\"\n",
    "        s3 = \"\"\"  Validation losses {:.4f} = cross entropy {:.4f} + KL {:.4f}, ({:.4f})\\n\"\"\"\n",
    "        print(s1.format(epoch + 1, epoch_duration / 60))\n",
    "        tlosses = losses['epoch ' + str(epoch)]['tlosses']\n",
    "        print(s2.format(tlosses['total'].mean(), tlosses['cross_entropy'].mean(),\n",
    "                        tlosses['kl'].mean(), tlosses['kl_prescale'].mean()))\n",
    "        vlosses = losses['epoch ' + str(epoch)]['vlosses']\n",
    "        print(s3.format(vlosses['total'].mean(), vlosses['cross_entropy'].mean(),\n",
    "                        vlosses['kl'].mean(), vlosses['kl_prescale'].mean()))\n",
    "\n",
    "        # write metrics to tensorboard\n",
    "        writer.scalar('loss (train)', tlosses['total'].mean(), epoch)\n",
    "        writer.scalar('cross entropy (train)', tlosses['cross_entropy'].mean(), epoch)\n",
    "        writer.scalar('KL (train)', tlosses['kl'].mean(), epoch)\n",
    "        writer.scalar('KL prescale (train)', tlosses['kl_prescale'].mean(), epoch)\n",
    "        writer.scalar('loss (validation)', vlosses['total'].mean(), epoch)\n",
    "        writer.scalar('cross entropy (validation)', vlosses['cross_entropy'].mean(), epoch)\n",
    "        writer.scalar('KL (validation)', vlosses['kl'].mean(), epoch)\n",
    "        writer.scalar('KL prescale (validation)', vlosses['kl_prescale'].mean(), epoch)\n",
    "        writer.flush()\n",
    "        \n",
    "        # save checkpoint\n",
    "        ckpt = {'params': params, 'optimizer_state': optimizer_state, 'losses': losses}\n",
    "        checkpoints.save_checkpoint(ckpt_dir = ckpt_dir, target = ckpt, step = epoch)\n",
    "        \n",
    "        # break if early stopping criteria met\n",
    "        _, early_stop = early_stop.update(vlosses['total'].mean())\n",
    "        if early_stop.should_stop:\n",
    "            \n",
    "            print('Early stopping criteria met, breaking...')\n",
    "            \n",
    "            break\n",
    "            \n",
    "    return params, optimizer_state, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aedcd70-655c-4fa2-9b32-fca3236ded0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use xeus-python kernel -- Python 3.9 (XPython) -- for debugging\n",
    "# breakpoint()\n",
    "# typing help at a breakpoint gives you list of available commands\n",
    "from jax.config import config\n",
    "config.update(\"jax_debug_nans\", True)\n",
    "config.update(\"jax_disable_jit\", False)\n",
    "\n",
    "from flax.metrics import tensorboard\n",
    "log_folder = \"runs/exp9/profile\"\n",
    "writer = tensorboard.SummaryWriter(log_folder)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs/exp9\n",
    "\n",
    "ckpt_dir = 'tmp/flax-checkpointing'\n",
    "\n",
    "import shutil\n",
    "if os.path.exists(ckpt_dir):\n",
    "    shutil.rmtree(ckpt_dir)  # remove any existing checkpoints from the last notebook run.\n",
    "\n",
    "trained_params, optimizer_state, losses = \\\n",
    "optimize_dynamical_VAE(init_params, x0, hyperparams, models, training_data, validation_data, \n",
    "                       optimisation_hyperparams, key, ckpt_dir, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ab104-b63e-4763-aa58-63bdc59e73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run model and plot\n",
    "# T = 100\n",
    "# data = training_data[0,:,:]\n",
    "# hyperparams['dt'] = 0.01\n",
    "# hyperparams['time_steps'] = 1000\n",
    "\n",
    "# params = init_params\n",
    "# results_encode = encode(models, params, data[None,:,:])\n",
    "# _, z_sampler, _ = models\n",
    "# z1, z2 = z_sampler(results_encode, params, hyperparams, key)\n",
    "# A = construct_dynamics_matrix(params, hyperparams)\n",
    "# results_decode = batch_decode(params, hyperparams, models, A, x0, T, z1, z2)\n",
    "# results = results_encode | results_decode\n",
    "\n",
    "# for ex in range(1):\n",
    "#     plt.scatter(results['pen_xy'][ex][:,0],results['pen_xy'][ex][:,1], c = 'k', alpha =  np.exp(results['pen_down_log_p'][ex,:]))\n",
    "# plt.ylim(0,105)\n",
    "# plt.xlim(0,105)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063782c5-ab5e-48b0-9c49-bfa90a2fc5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # restore checkpoint\n",
    "# ckpt = {'params': trained_params, 'optimizer_state': optimizer_state, 'losses': losses}\n",
    "# restored_state = checkpoints.restore_checkpoint(ckpt_dir = ckpt_dir, target = ckpt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
